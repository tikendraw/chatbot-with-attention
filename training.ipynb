{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tikendraw/chatbot-with-attention/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "581c485f-6b99-4710-a4db-9b7a5b1953be",
      "metadata": {
        "id": "581c485f-6b99-4710-a4db-9b7a5b1953be"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "de9895a2-8aa5-4a97-8c09-537f95dc8648",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de9895a2-8aa5-4a97-8c09-537f95dc8648",
        "outputId": "63b8a0b8-5c31-45ce-976f-7a135f20f609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Cloning into 'chatbot-with-attention'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 48 (delta 7), reused 39 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (48/48), 35.30 MiB | 7.25 MiB/s, done.\n",
            "/content/chatbot-with-attention\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    \n",
        "    # Mount Google drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    ! git clone https://github.com/tikendraw/chatbot-with-attention.git \n",
        "    os.chdir('chatbot-with-attention') \n",
        "    print(os.getcwd())\n",
        "\n",
        "    ! pip install tensorflow==2.11 -q\n",
        "    ! pip install tensorflow-text -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f021dd31-9ec0-4039-bbe7-10271ec9640a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f021dd31-9ec0-4039-bbe7-10271ec9640a",
        "outputId": "1846a19b-570a-481c-e5ef-802c5386f68f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Avaliable:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    TextVectorization, \n",
        "    Embedding, \n",
        "    LSTM, \n",
        "    GRU, \n",
        "    Bidirectional, \n",
        "    TimeDistributed, \n",
        "    Dense, \n",
        "    Attention, \n",
        "    MultiHeadAttention\n",
        ")\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "print('GPU Avaliable: ', gpu:=len(tf.config.list_physical_devices('GPU')))\n",
        "if gpu:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d3bc845-ba26-4330-9ed5-cda72941e51c",
      "metadata": {
        "id": "3d3bc845-ba26-4330-9ed5-cda72941e51c"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "61c8b316-519e-4baf-931d-14fe9ce2dc58",
      "metadata": {
        "id": "61c8b316-519e-4baf-931d-14fe9ce2dc58"
      },
      "outputs": [],
      "source": [
        "MAX_OUTPUT_LENGTH = 102\n",
        "BATCH_SIZE = 32\n",
        "UNITS = 64\n",
        "EMBEDDING_DIMS = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3806af9-4f24-42e4-ba56-20c4e3f386aa",
      "metadata": {
        "id": "e3806af9-4f24-42e4-ba56-20c4e3f386aa"
      },
      "source": [
        "# Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cb91ce87-6d26-48a6-adc7-c83c584495f5",
      "metadata": {
        "id": "cb91ce87-6d26-48a6-adc7-c83c584495f5"
      },
      "outputs": [],
      "source": [
        "# preprocessing text\n",
        "def tf_lower_and_split_punct_en(text):\n",
        "    # Split accented characters.\n",
        "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "    text = tf.strings.lower(text)\n",
        "    # Keep space, a to z, and select punctuation.\n",
        "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "    # Add spaces around punctuation.\n",
        "    text = tf.strings.regex_replace(text, '[.?!,¿|]', r' \\0 ')\n",
        "    # Strip whitespace.\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a58406b5-b0ae-4c65-952a-d43e27e34c5a",
      "metadata": {
        "id": "a58406b5-b0ae-4c65-952a-d43e27e34c5a"
      },
      "outputs": [],
      "source": [
        "# Loading vectorizer\n",
        "from_disk = pickle.load(open(\"./components/vectorizer.pkl\", \"rb\"))\n",
        "vectorizer = TextVectorization.from_config(from_disk['config'])\n",
        "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "vectorizer.set_weights(from_disk['weights'])\n",
        "\n",
        "# Lets see the Vector for word \"this\"\n",
        "# print (vectorizer(\"who am i\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb0dc2a-07fc-4094-b47f-4e82316806b6",
      "metadata": {
        "id": "dbb0dc2a-07fc-4094-b47f-4e82316806b6"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a260a269-ff87-4944-9573-1ccad9210800",
      "metadata": {
        "id": "a260a269-ff87-4944-9573-1ccad9210800"
      },
      "outputs": [],
      "source": [
        "save_train_data_path = './dataset/train/'\n",
        "save_test_data_path = './dataset/test/'\n",
        "\n",
        "#loading the data\n",
        "train_data = tf.data.Dataset.load(save_train_data_path, compression='GZIP')\n",
        "test_data = tf.data.Dataset.load(save_test_data_path, compression='GZIP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a06b2375-303c-4c5a-8e3a-2019782cdfd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a06b2375-303c-4c5a-8e3a-2019782cdfd2",
        "outputId": "a77d249f-05e4-46c8-ea3d-381f0989c3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input\n",
            "[    3    20 10942   120     5  1994    11  6126    14  9357  5660   154\n",
            "     9  7211     2   443   532   105    61 11786]\n",
            "--------------------------------------------\n",
            "decoder input\n",
            "[   3 1361  174   13 3319 4872   69   39   11 1415  764  922    2    4\n",
            "    0    0    0    0    0    0]\n",
            "--------------------------------------------\n",
            "encoder output\n",
            "[1361  174   13 3319 4872   69   39   11 1415  764  922    2    4    0\n",
            "    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "for (enc_input, dec_input), dec_output  in train_data.take(1):\n",
        "    print('encoder input')\n",
        "    print(enc_input[0, :20].numpy())\n",
        "    print('-'*44)\n",
        "    print('decoder input')\n",
        "    print(dec_input[0, :20].numpy()) \n",
        "    print('-'*44)\n",
        "    print('encoder output')\n",
        "    print(dec_output[0, :20].numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "23b273d7-3da9-4990-847b-8564d83af25b",
      "metadata": {
        "id": "23b273d7-3da9-4990-847b-8564d83af25b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "acf3eb4b-2d2d-4df3-96a5-2a496bfefa63",
      "metadata": {
        "id": "acf3eb4b-2d2d-4df3-96a5-2a496bfefa63"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a9adb19a-528b-495c-bfa2-e6c693afc8c3",
      "metadata": {
        "id": "a9adb19a-528b-495c-bfa2-e6c693afc8c3"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, x, context):\n",
        "\n",
        "        attn_output, attn_scores = self.mha(\n",
        "            query=x,\n",
        "            value=context,\n",
        "            return_attention_scores=True)\n",
        "\n",
        "        # Cache the attention scores for plotting later.\n",
        "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "        self.last_attention_weights = attn_scores\n",
        "\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c6eeb3-3f2b-4292-a206-a7abd0c07218",
      "metadata": {
        "id": "43c6eeb3-3f2b-4292-a206-a7abd0c07218"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "af526d19-6fc6-48fe-8470-4789bc715701",
      "metadata": {
        "id": "af526d19-6fc6-48fe-8470-4789bc715701"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, text_vectorizer, units, embed_dims):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.text_vectorizer =  text_vectorizer\n",
        "        self.units = units\n",
        "        self.embed_dims = embed_dims\n",
        "        self.vocab_size = text_vectorizer.vocabulary_size()\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embed_dims, mask_zero=True, )\n",
        "        self.rnn = Bidirectional(merge_mode='concat', layer = GRU(self.units, return_sequences=True, return_state=True))\n",
        "        \n",
        "    def call(self, x, y=None, return_state=False):\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        encoder_output, encoder_fw_state, encoder_bw_state = self.rnn(x)\n",
        "        # encoder_state = [encoder_fw_state, encoder_bw_state]  # for LSTM\n",
        "        encoder_state = encoder_fw_state                        # for GRU\n",
        "\n",
        "        if return_state:\n",
        "            return encoder_output, encoder_state\n",
        "        else:\n",
        "            return encoder_output\n",
        "        \n",
        "    def convert_input(self, texts, return_state=False):\n",
        "        texts = tf.convert_to_tensor(texts)\n",
        "        if len(texts.shape) == 0:\n",
        "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "        context = self.text_vectorizer(texts)\n",
        "        \n",
        "        context = self(context, return_state = return_state)\n",
        "        \n",
        "        return context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242c26be-d2b5-4d45-90d0-cad07e0d6496",
      "metadata": {
        "id": "242c26be-d2b5-4d45-90d0-cad07e0d6496"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "a39d0588-f0e0-4fe3-bdca-d24cc834b802",
      "metadata": {
        "id": "a39d0588-f0e0-4fe3-bdca-d24cc834b802"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "    def __init__(self, text_vectorizer, units,  embed_dims) :\n",
        "        super(Decoder, self).__init__()\n",
        "        self.text_vectorizer =  text_vectorizer\n",
        "        self.units = units\n",
        "        self.embed_dims = embed_dims\n",
        "        self.vocab_size = text_vectorizer.vocabulary_size()\n",
        "        \n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embed_dims, mask_zero=True, )\n",
        "        self.rnn = GRU(self.units, return_sequences=True, return_state=True)\n",
        "        \n",
        "        # self.attention =  tf.keras.layers.Attention()\n",
        "        self.attention = CrossAttention(units)\n",
        "        \n",
        "        self.output_dense = Dense(self.vocab_size)\n",
        "        \n",
        "        self.word_to_id = tf.keras.layers.StringLookup(vocabulary=text_vectorizer.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "        self.id_to_word = tf.keras.layers.StringLookup(vocabulary=text_vectorizer.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
        "        \n",
        "        self.start_token = self.word_to_id('[START]')\n",
        "        self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    def call(self, x, context, state=None, return_state = False):\n",
        "        ''' x, context, state=None, return_sequence=False '''\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        decoder_output, decoder_state = self.rnn(x, initial_state=state)\n",
        "        \n",
        "        # Simple attetion\n",
        "        x = self.attention([decoder_output, context])\n",
        "        \n",
        "        # decoder_state = [decoder_state_h, decoder_state_c]\n",
        "        x = self.attention(decoder_output, context)\n",
        "        self.last_attention_weights = self.attention.last_attention_weights\n",
        "\n",
        "        logits = self.output_dense(x)\n",
        "        \n",
        "        if return_state:\n",
        "            return logits, decoder_state\n",
        "        else:\n",
        "            return logits\n",
        "        \n",
        "    def get_initial_state(self, context):\n",
        "        batch_size = tf.shape(context)[0]\n",
        "        start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "        embedded = self.embedding(start_tokens)\n",
        "        return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
        "\n",
        "    \n",
        "    def tokens_to_text(self, tokens):\n",
        "        words = self.id_to_word(tokens)\n",
        "        result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "        result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "        result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "        return result\n",
        "    \n",
        "    def get_next_token(self, next_token, context,  done, state, temperature = 0.0):\n",
        "        \n",
        "        logits, state = self(next_token, context, state = state, return_state=True) \n",
        "\n",
        "        if temperature == 0.0:\n",
        "            next_token = tf.argmax(logits, axis=-1)\n",
        "        else:\n",
        "            logits = logits[:, -1, :]/temperature\n",
        "            next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "        # If a sequence produces an `end_token`, set it `done`\n",
        "        done = done | (next_token == self.end_token)\n",
        "        # Once a sequence is done it only produces 0-padding.\n",
        "        next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "        return next_token, done, state"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rlpxYQpkdrZc"
      },
      "id": "rlpxYQpkdrZc",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rnn = GRU(1)\n",
        "# rnn1 = LSTM(1)\n",
        "\n",
        "# ee = decoder.embedding(enc_input)\n",
        "# print(ee.shape)\n",
        "\n",
        "# ree = rnn1(ee)\n",
        "# print(ree.shape)\n",
        "\n",
        "# encoder = Encoder(vectorizer, UNITS, EMBEDDING_DIMS)\n",
        "# enc_context, enc_state = encoder(enc_input, return_state = True)\n",
        "\n",
        "# print('enc_input:', enc_input.shape)\n",
        "# print('enc_context:', enc_context.shape)\n",
        "# print('enc_state:', enc_state.shape)\n",
        "\n",
        "# decoder = Decoder(vectorizer, UNITS, EMBEDDING_DIMS)\n",
        "# dec_out, dec_state = decoder(dec_input, enc_context, return_state = True )\n",
        "\n",
        "# print('dec_input:', dec_input.shape)\n",
        "# print('dec_out:', dec_out.shape)\n",
        "# print('dec_state:', dec_state.shape)b"
      ],
      "metadata": {
        "id": "YkeAeaVXdxJQ"
      },
      "id": "YkeAeaVXdxJQ",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re = tf.expand_dims(ree, -1)\n",
        "reee = keras.layers.RepeatVector(4)(re)\n",
        "reee.shape, ree.shape, re.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "vGGphxLphAcz",
        "outputId": "f73ebaba-d81c-407b-a9d2-eea7b552c509"
      },
      "id": "vGGphxLphAcz",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-6cfb2bd39580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreee\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    233\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"repeat_vector_3\" is incompatible with the layer: expected ndim=2, found ndim=3. Full shape received: (32, 1, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cecfb5f9-66b3-49ea-8bd1-2d9f025a7d7e",
      "metadata": {
        "id": "cecfb5f9-66b3-49ea-8bd1-2d9f025a7d7e"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "1b8acf32-bfc7-4e09-b669-bf6a1d7e1e36",
      "metadata": {
        "id": "1b8acf32-bfc7-4e09-b669-bf6a1d7e1e36"
      },
      "outputs": [],
      "source": [
        "class ChatBot(tf.keras.Model):\n",
        "    \n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self, text_processor, units, embed_dims):\n",
        "        super().__init__()\n",
        "        self.text_processor = text_processor\n",
        "        self.units = units\n",
        "        self.embed_dims = embed_dims\n",
        "        \n",
        "        # Build the encoder and decoder\n",
        "        encoder = Encoder(text_processor, units, embed_dims)\n",
        "        decoder = Decoder(text_processor, units, embed_dims)\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        context, x = inputs\n",
        "        context , state= self.encoder(context, return_state = True)\n",
        "        logits = self.decoder(x, context, state)\n",
        "\n",
        "        #TODO(b/250038731): remove this\n",
        "        try:\n",
        "          # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "            del logits._keras_mask\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "377ba89b-626c-48ed-922d-59579a3f4f8c",
      "metadata": {
        "id": "377ba89b-626c-48ed-922d-59579a3f4f8c"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "cea82227-e78f-426e-a082-5934bab185b2",
      "metadata": {
        "id": "cea82227-e78f-426e-a082-5934bab185b2"
      },
      "outputs": [],
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f541cf8-23a9-4dc7-b2a6-2a71293d7a20",
      "metadata": {
        "id": "5f541cf8-23a9-4dc7-b2a6-2a71293d7a20"
      },
      "source": [
        "# Compile and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "4ea05d9d-b480-42b9-85bb-e7509eff5285",
      "metadata": {
        "id": "4ea05d9d-b480-42b9-85bb-e7509eff5285"
      },
      "outputs": [],
      "source": [
        "model = ChatBot(vectorizer, UNITS, EMBEDDING_DIMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "7067ba74-31a8-4872-b12d-851b4474c2c2",
      "metadata": {
        "id": "7067ba74-31a8-4872-b12d-851b4474c2c2"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=masked_loss, \n",
        "              metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "e72f685c-28cb-4477-8aa7-6d45784a35f3",
      "metadata": {
        "id": "e72f685c-28cb-4477-8aa7-6d45784a35f3"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "CKPT_DIR = './model_checkpoint'\n",
        "# CKPT_DIR = '/content/drive/MyDrive/tf_model/chatbot'\n",
        "os.makedirs(CKPT_DIR, exist_ok = True)\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    os.path.join(CKPT_DIR,  f\"{datetime.now().strftime('%m:%d:%Y, %H:%M:%S')}\"),\n",
        "    monitor= 'masked_acc',\n",
        "    verbose= 0,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,\n",
        "    mode= 'auto',\n",
        "    save_freq='epoch'\n",
        ")\n",
        "\n",
        "os.makedirs('log', exist_ok = True)\n",
        "csv_logger = CSVLogger('./log/training.log')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "bfcfa8af-e58d-41ed-896b-25686db9f034",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfcfa8af-e58d-41ed-896b-25686db9f034",
        "outputId": "c8661a35-57a6-4b87-e8a8-12ff8dbcb4d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "embedding shape:  (None, None, 128)\n",
            "rnn shape:  (None, None, 64)\n",
            "embedding shape:  (None, None, 128)\n",
            "rnn shape:  (None, None, 64)\n",
            "80/80 [==============================] - ETA: 0s - loss: 7.7633 - masked_acc: 0.1180 - masked_loss: 7.7633embedding shape:  (None, None, 128)\n",
            "rnn shape:  (None, None, 64)\n",
            "80/80 [==============================] - 53s 461ms/step - loss: 7.7633 - masked_acc: 0.1180 - masked_loss: 7.7633 - val_loss: 6.0512 - val_masked_acc: 0.1182 - val_masked_loss: 6.0512\n",
            "Epoch 2/10\n",
            "80/80 [==============================] - 29s 356ms/step - loss: 5.8547 - masked_acc: 0.1359 - masked_loss: 5.8547 - val_loss: 5.8132 - val_masked_acc: 0.1653 - val_masked_loss: 5.8132\n",
            "Epoch 3/10\n",
            "80/80 [==============================] - 25s 311ms/step - loss: 5.5484 - masked_acc: 0.1741 - masked_loss: 5.5484 - val_loss: 5.5597 - val_masked_acc: 0.1802 - val_masked_loss: 5.5597\n",
            "Epoch 4/10\n",
            "80/80 [==============================] - 24s 297ms/step - loss: 5.3707 - masked_acc: 0.1888 - masked_loss: 5.3707 - val_loss: 5.3556 - val_masked_acc: 0.2032 - val_masked_loss: 5.3556\n",
            "Epoch 5/10\n",
            "80/80 [==============================] - 23s 291ms/step - loss: 5.1955 - masked_acc: 0.2135 - masked_loss: 5.1955 - val_loss: 5.2262 - val_masked_acc: 0.2178 - val_masked_loss: 5.2262\n",
            "Epoch 6/10\n",
            "80/80 [==============================] - 24s 298ms/step - loss: 5.0756 - masked_acc: 0.2247 - masked_loss: 5.0756 - val_loss: 5.1356 - val_masked_acc: 0.2230 - val_masked_loss: 5.1356\n",
            "Epoch 7/10\n",
            "80/80 [==============================] - 22s 282ms/step - loss: 4.9810 - masked_acc: 0.2349 - masked_loss: 4.9810 - val_loss: 5.0890 - val_masked_acc: 0.2255 - val_masked_loss: 5.0890\n",
            "Epoch 8/10\n",
            "80/80 [==============================] - 22s 272ms/step - loss: 4.9706 - masked_acc: 0.2303 - masked_loss: 4.9706 - val_loss: 5.0324 - val_masked_acc: 0.2291 - val_masked_loss: 5.0324\n",
            "Epoch 9/10\n",
            "80/80 [==============================] - 23s 289ms/step - loss: 4.8745 - masked_acc: 0.2368 - masked_loss: 4.8745 - val_loss: 4.9787 - val_masked_acc: 0.2310 - val_masked_loss: 4.9787\n",
            "Epoch 10/10\n",
            "80/80 [==============================] - 23s 285ms/step - loss: 4.8476 - masked_acc: 0.2425 - masked_loss: 4.8476 - val_loss: 4.9223 - val_masked_acc: 0.2409 - val_masked_loss: 4.9223\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "history = model.fit(\n",
        "    train_data.repeat(), \n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch = 80,\n",
        "    validation_data=test_data,\n",
        "    validation_steps = 3,\n",
        "    callbacks=[\n",
        "                tf.keras.callbacks.EarlyStopping(patience=5),\n",
        "                model_ckpt,\n",
        "                csv_logger]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa3096e-6f18-465a-ba4f-72cb3af03086",
      "metadata": {
        "id": "efa3096e-6f18-465a-ba4f-72cb3af03086"
      },
      "source": [
        "# Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "2ec71adf-c81f-45a2-a0e6-46cbe8cbfaff",
      "metadata": {
        "id": "2ec71adf-c81f-45a2-a0e6-46cbe8cbfaff"
      },
      "outputs": [],
      "source": [
        "@ChatBot.add_method\n",
        "def translate(self,\n",
        "              texts, *,\n",
        "              max_length=50,\n",
        "              temperature=0.0):\n",
        "    # Process the input texts\n",
        "    context = self.encoder.convert_input(texts, return_state = True)\n",
        "\n",
        "    context, enc_state = context\n",
        "    # fw_state, bw_state = state\n",
        "    batch_size = tf.shape(texts)[0]\n",
        "\n",
        "    # Setup the loop inputs\n",
        "    tokens = []\n",
        "    attention_weights = []\n",
        "    next_token, done, state = self.decoder.get_initial_state(context)\n",
        "    state = enc_state\n",
        "    # state =[state,state]\n",
        "    for _ in range(max_length):\n",
        "        # Generate the next token\n",
        "        next_token, done, state = self.decoder.get_next_token(\n",
        "                next_token, context, done,  state, temperature)\n",
        "\n",
        "        # Collect the generated tokens\n",
        "        tokens.append(next_token)\n",
        "        # attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "            break\n",
        "\n",
        "    # Stack the lists of tokens and attention weights.\n",
        "    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "    # self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "    result = self.decoder.tokens_to_text(tokens)\n",
        "    return result, tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "f7709f55-9f9f-4d37-b071-975678c5e045",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7709f55-9f9f-4d37-b071-975678c5e045",
        "outputId": "6c247245-8aca-4f9d-8ef6-1543112a60be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding shape:  (1, 1, 128)\n",
            "rnn shape:  (1, 1, 64)\n",
            "embedding shape:  (1, 1, 128)\n",
            "rnn shape:  (1, 1, 64)\n",
            "embedding shape:  (1, 1, 128)\n",
            "rnn shape:  (1, 1, 64)\n",
            "embedding shape:  (1, 1, 128)\n",
            "rnn shape:  (1, 1, 64)\n",
            "embedding shape:  (1, 1, 128)\n",
            "rnn shape:  (1, 1, 64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'i dont know . '"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "result = model.translate(['How long you have been there?'], temperature = 0)\n",
        "result[0].numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c4edd67c-a52a-4dae-97ad-7492a234d26b",
      "metadata": {
        "id": "c4edd67c-a52a-4dae-97ad-7492a234d26b"
      },
      "outputs": [],
      "source": [
        "# model1 = model  # model with attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3cc8e7f2-b6fe-4782-941f-dfffb8e060a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cc8e7f2-b6fe-4782-941f-dfffb8e060a0",
        "outputId": "4d6f9c1a-7325-442f-d088-032af57a9fc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'i dont know . '], dtype=object)>,\n",
              " <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[ 8, 23, 27,  2,  0]])>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c7934f0-9567-4cc8-a07a-0c956220e124",
      "metadata": {
        "id": "2c7934f0-9567-4cc8-a07a-0c956220e124"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c09f75-88d5-4dc4-9a42-398b7a17a4d0",
      "metadata": {
        "id": "13c09f75-88d5-4dc4-9a42-398b7a17a4d0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b586d8e-0b41-4f2c-b20a-f6e05aa051dc",
      "metadata": {
        "id": "2b586d8e-0b41-4f2c-b20a-f6e05aa051dc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7447dbf-c935-4080-8230-c827a38e98d1",
      "metadata": {
        "id": "b7447dbf-c935-4080-8230-c827a38e98d1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d52f1642-700a-42ec-900f-41c19dc9cf97",
      "metadata": {
        "id": "d52f1642-700a-42ec-900f-41c19dc9cf97"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f40775f-25aa-45e9-9b2c-5ef2a47bafee",
      "metadata": {
        "id": "3f40775f-25aa-45e9-9b2c-5ef2a47bafee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8add436-3533-41ca-b864-63a85f5a30c2",
      "metadata": {
        "id": "d8add436-3533-41ca-b864-63a85f5a30c2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4158bb21-094d-4bd9-bf1f-8a209d980676",
      "metadata": {
        "id": "4158bb21-094d-4bd9-bf1f-8a209d980676"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4b7996-8c34-4c51-a707-60caf8921a84",
      "metadata": {
        "id": "de4b7996-8c34-4c51-a707-60caf8921a84"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f366abef-cd80-42bb-b680-ad634d234eb9",
      "metadata": {
        "id": "f366abef-cd80-42bb-b680-ad634d234eb9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d8fb6b-0362-4a39-b06b-af08bee0cf6d",
      "metadata": {
        "id": "38d8fb6b-0362-4a39-b06b-af08bee0cf6d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13198b71-3dcf-4218-94b1-994db06e7b89",
      "metadata": {
        "id": "13198b71-3dcf-4218-94b1-994db06e7b89"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98e0ed1-7a81-4793-91a1-6839b9864bd8",
      "metadata": {
        "id": "f98e0ed1-7a81-4793-91a1-6839b9864bd8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61097607-493e-4ada-979b-492f874d0ded",
      "metadata": {
        "id": "61097607-493e-4ada-979b-492f874d0ded"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}