{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tikendraw/chatbot-with-attention/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatBot Training\n",
        "\n"
      ],
      "metadata": {
        "id": "581c485f-6b99-4710-a4db-9b7a5b1953be"
      },
      "id": "581c485f-6b99-4710-a4db-9b7a5b1953be"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "de9895a2-8aa5-4a97-8c09-537f95dc8648",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de9895a2-8aa5-4a97-8c09-537f95dc8648",
        "outputId": "e3ed0aa9-5415-4e08-c1ab-b44fd6cbe13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chatbot-with-attention'...\n",
            "remote: Enumerating objects: 229, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 229 (delta 14), reused 49 (delta 13), pack-reused 176\u001b[K\n",
            "Receiving objects: 100% (229/229), 220.48 MiB | 17.44 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n",
            "Updating files: 100% (37/37), done.\n",
            "/content/chatbot-with-attention\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    \n",
        "    # Mount Google drive\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    \n",
        "    ! git clone https://github.com/tikendraw/chatbot-with-attention.git \n",
        "    os.chdir('chatbot-with-attention') \n",
        "    print(os.getcwd())\n",
        "\n",
        "    ! pip install tensorflow==2.11 -q\n",
        "    ! pip install tensorflow-text -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f021dd31-9ec0-4039-bbe7-10271ec9640a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f021dd31-9ec0-4039-bbe7-10271ec9640a",
        "outputId": "16c9c619-d611-4329-c6a7-f79afc80e7bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Avaliable:  1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    TextVectorization, \n",
        "    Embedding, \n",
        "    LSTM, \n",
        "    GRU, \n",
        "    Bidirectional, \n",
        "    TimeDistributed, \n",
        "    Dense, \n",
        "    Attention, \n",
        "    MultiHeadAttention,\n",
        "    Concatenate\n",
        ")\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "print('GPU Avaliable: ', gpu:=len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d3bc845-ba26-4330-9ed5-cda72941e51c",
      "metadata": {
        "id": "3d3bc845-ba26-4330-9ed5-cda72941e51c"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "61c8b316-519e-4baf-931d-14fe9ce2dc58",
      "metadata": {
        "id": "61c8b316-519e-4baf-931d-14fe9ce2dc58"
      },
      "outputs": [],
      "source": [
        "MAX_OUTPUT_LENGTH = 200\n",
        "BATCH_SIZE = 32\n",
        "UNITS = 64\n",
        "EMBEDDING_DIMS = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3806af9-4f24-42e4-ba56-20c4e3f386aa",
      "metadata": {
        "id": "e3806af9-4f24-42e4-ba56-20c4e3f386aa"
      },
      "source": [
        "# Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cb91ce87-6d26-48a6-adc7-c83c584495f5",
      "metadata": {
        "id": "cb91ce87-6d26-48a6-adc7-c83c584495f5"
      },
      "outputs": [],
      "source": [
        "# preprocessing text\n",
        "def tf_lower_and_split_punct_en(text):\n",
        "    # Split accented characters.\n",
        "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "    text = tf.strings.lower(text)\n",
        "    # Keep space, a to z, and select punctuation.\n",
        "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "    # Add spaces around punctuation.\n",
        "    text = tf.strings.regex_replace(text, '[.?!,¿|]', r' \\0 ')\n",
        "    # Strip whitespace.\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a58406b5-b0ae-4c65-952a-d43e27e34c5a",
      "metadata": {
        "id": "a58406b5-b0ae-4c65-952a-d43e27e34c5a"
      },
      "outputs": [],
      "source": [
        "# Loading vectorizer\n",
        "from_disk = pickle.load(open(\"./components/vectorizer.pkl\", \"rb\"))\n",
        "vectorizer = TextVectorization.from_config(from_disk['config'])\n",
        "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "vectorizer.set_weights(from_disk['weights'])\n",
        "\n",
        "# Lets see the Vector for word \"this\"\n",
        "# print (vectorizer(\"who am i\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217530b2-25b8-4622-a415-e36f31306d1a",
      "metadata": {
        "id": "217530b2-25b8-4622-a415-e36f31306d1a"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0e1144e5-e080-4366-88f6-bd52f5b0b154",
      "metadata": {
        "id": "0e1144e5-e080-4366-88f6-bd52f5b0b154"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "32CCNnGzXTmF",
        "outputId": "9e7d5d33-85a3-42fe-c2ee-da6dc6c8a37c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "32CCNnGzXTmF",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " components\t\t\t    LICENSE\n",
            "'cornell movie-dialogs corpus'\t    movie_dialogue_chatbot.ipynb\n",
            " cornell_movie_dialogs_corpus.zip   preprocessing_cornelldata.py\n",
            " dataset\t\t\t    preprocess.py\n",
            " dataset.csv\t\t\t    README.md\n",
            " embedding\t\t\t    TextFile.txt\n",
            " funcyou\t\t\t    training.ipynb\n",
            " libdevice.10.bc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zipf = zipfile.ZipFile('./embedding/embedding_matrix.zip')\n",
        "zipf.extractall('./embedding/')\n",
        "zipf.close()"
      ],
      "metadata": {
        "id": "1OCHcqCdXdOU"
      },
      "id": "1OCHcqCdXdOU",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f8fbbc0a-dfaf-4b25-84aa-7592ab122c6d",
      "metadata": {
        "id": "f8fbbc0a-dfaf-4b25-84aa-7592ab122c6d"
      },
      "outputs": [],
      "source": [
        "# Loading embedding_matrix\n",
        "embedding_matrix = np.load('./embedding/embedding_matrix.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1a98d151-06af-416d-a9ca-33a1ad9ce42b",
      "metadata": {
        "id": "1a98d151-06af-416d-a9ca-33a1ad9ce42b",
        "outputId": "d85e8b6c-366f-4fc1-807c-e79bc8ba4c9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59905, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "da7b08f1-373d-4866-b2ae-7ce56f43fb77",
      "metadata": {
        "id": "da7b08f1-373d-4866-b2ae-7ce56f43fb77"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dbb0dc2a-07fc-4094-b47f-4e82316806b6",
      "metadata": {
        "id": "dbb0dc2a-07fc-4094-b47f-4e82316806b6"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a260a269-ff87-4944-9573-1ccad9210800",
      "metadata": {
        "id": "a260a269-ff87-4944-9573-1ccad9210800"
      },
      "outputs": [],
      "source": [
        "save_train_data_path = './dataset/train/'\n",
        "save_test_data_path = './dataset/test/'\n",
        "\n",
        "#loading the data\n",
        "train_data = tf.data.Dataset.load(save_train_data_path, compression='GZIP')\n",
        "test_data = tf.data.Dataset.load(save_test_data_path, compression='GZIP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a06b2375-303c-4c5a-8e3a-2019782cdfd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a06b2375-303c-4c5a-8e3a-2019782cdfd2",
        "outputId": "c7a63c06-ac75-4a31-9736-54025a3873f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input\n",
            "[    3    58    33   115    22   993     7 48680 52632    13  4132  8573\n",
            "    40   421    82  3902 24129   978   516    54]\n",
            "--------------------------------------------\n",
            "decoder input\n",
            "[    3    58    33   115    22   993     7 48680 52632    13  4132  8573\n",
            "    40   421    82  3902 24129   978   516    54]\n",
            "--------------------------------------------\n",
            "encoder output\n",
            "[   58    33   115    22   993     7 48680 52632    13  4132  8573    40\n",
            "   421    82  3902 24129   978   516    54    36]\n"
          ]
        }
      ],
      "source": [
        "for (enc_input, dec_input), dec_output  in train_data.take(1):\n",
        "    print('encoder input')\n",
        "    print(enc_input[0, :20].numpy())\n",
        "    print('-'*44)\n",
        "    print('decoder input')\n",
        "    print(dec_input[0, :20].numpy()) \n",
        "    print('-'*44)\n",
        "    print('encoder output')\n",
        "    print(dec_output[0, :20].numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "23b273d7-3da9-4990-847b-8564d83af25b",
      "metadata": {
        "id": "23b273d7-3da9-4990-847b-8564d83af25b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "acf3eb4b-2d2d-4df3-96a5-2a496bfefa63",
      "metadata": {
        "id": "acf3eb4b-2d2d-4df3-96a5-2a496bfefa63"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a9adb19a-528b-495c-bfa2-e6c693afc8c3",
      "metadata": {
        "id": "a9adb19a-528b-495c-bfa2-e6c693afc8c3"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, x, context):\n",
        "\n",
        "        attn_output, attn_scores = self.mha(\n",
        "            query=x,\n",
        "            value=context,\n",
        "            return_attention_scores=True)\n",
        "\n",
        "        # Cache the attention scores for plotting later.\n",
        "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "        self.last_attention_weights = attn_scores\n",
        "\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9M2auobvj7F0",
      "metadata": {
        "id": "9M2auobvj7F0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "86c16205-0b6a-4e4a-8551-5fe31ed729da",
      "metadata": {
        "id": "86c16205-0b6a-4e4a-8551-5fe31ed729da"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c6eeb3-3f2b-4292-a206-a7abd0c07218",
      "metadata": {
        "id": "43c6eeb3-3f2b-4292-a206-a7abd0c07218"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "af526d19-6fc6-48fe-8470-4789bc715701",
      "metadata": {
        "id": "af526d19-6fc6-48fe-8470-4789bc715701"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, text_vectorizer, units, embed_dims, embedding_matrix=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.text_vectorizer =  text_vectorizer\n",
        "        self.units = units\n",
        "        self.embed_dims = embed_dims\n",
        "        self.vocab_size = text_vectorizer.vocabulary_size()\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size , output_dim=self.embed_dims, mask_zero=True, trainable=False)\n",
        "        self.embedding.build((None,))\n",
        "        self.embedding.set_weights([embedding_matrix])\n",
        "        self.rnn = Bidirectional(merge_mode='concat', layer = LSTM(self.units, return_sequences=True, return_state=True))\n",
        "        \n",
        "    def call(self, x, y=None, return_state=False):\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        encoder_output, forward_h, forward_c, backward_h, backward_c, = self.rnn(x)\n",
        "        \n",
        "        state_h = Concatenate()([forward_h, backward_h])\n",
        "        state_c = Concatenate()([forward_c, backward_c])\n",
        "        \n",
        "        encoder_state = [state_h, state_c]\n",
        "        \n",
        "        if return_state:\n",
        "            return encoder_output, encoder_state\n",
        "        else:\n",
        "            return encoder_output\n",
        "        \n",
        "    def convert_input(self, texts, return_state=False):\n",
        "        texts = tf.convert_to_tensor(texts)\n",
        "        if len(texts.shape) == 0:\n",
        "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "        context = self.text_vectorizer(texts)\n",
        "        \n",
        "        context = self(context, return_state = return_state)\n",
        "        \n",
        "        return context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242c26be-d2b5-4d45-90d0-cad07e0d6496",
      "metadata": {
        "id": "242c26be-d2b5-4d45-90d0-cad07e0d6496"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a39d0588-f0e0-4fe3-bdca-d24cc834b802",
      "metadata": {
        "id": "a39d0588-f0e0-4fe3-bdca-d24cc834b802"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "    def __init__(self, text_vectorizer, units,  embed_dims, embedding_matrix=None) :\n",
        "        super(Decoder, self).__init__()\n",
        "        self.text_vectorizer =  text_vectorizer\n",
        "        self.units = units * 2\n",
        "        self.embed_dims = embed_dims\n",
        "        self.vocab_size = text_vectorizer.vocabulary_size()\n",
        "        \n",
        "        self.embedding = Embedding(input_dim=self.vocab_size , output_dim=self.embed_dims, mask_zero=True, trainable=False)\n",
        "        self.embedding.build((None,))\n",
        "        self.embedding.set_weights([embedding_matrix])\n",
        "        \n",
        "        self.rnn = LSTM(self.units, return_sequences=True, return_state=True)\n",
        "        \n",
        "        # self.attention =  tf.keras.layers.Attention()\n",
        "        # self.attention = CrossAttention(units)\n",
        "        self.attention = AttentionLayer()\n",
        "        \n",
        "        self.output_dense = Dense(self.vocab_size)\n",
        "        \n",
        "        self.word_to_id = tf.keras.layers.StringLookup(vocabulary=text_vectorizer.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "        self.id_to_word = tf.keras.layers.StringLookup(vocabulary=text_vectorizer.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
        "        \n",
        "        self.start_token = self.word_to_id('[START]')\n",
        "        self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    def call(self, x, context, state=None, return_state = False, training=False):\n",
        "        ''' x, context, state=None, return_sequence=False '''\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        decoder_output, decoder_state_h, decoder_state_c = self.rnn(x, initial_state=state)\n",
        "        decoder_state = [decoder_state_h, decoder_state_c]\n",
        "        \n",
        "        # attention\n",
        "        attn_op, attn_state = self.attention([context, decoder_output])\n",
        "        x = Concatenate(axis=-1)([decoder_output, attn_op])\n",
        "        \n",
        "        # x = self.attention(decoder_output, context)\n",
        "        # self.last_attention_weights = self.attention.last_attention_weights\n",
        "\n",
        "        logits = self.output_dense(x)\n",
        "            \n",
        "        if return_state:\n",
        "            return logits, decoder_state\n",
        "        else:\n",
        "            return logits\n",
        "        \n",
        "    def get_initial_state(self, context):\n",
        "        batch_size = tf.shape(context)[0]\n",
        "        start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "        embedded = self.embedding(start_tokens)\n",
        "        return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
        "\n",
        "    \n",
        "    def tokens_to_text(self, tokens):\n",
        "        words = self.id_to_word(tokens)\n",
        "        result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "        result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "        result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "        return result\n",
        "    \n",
        "    def get_next_token(self, next_token, context,  done, state, temperature = 0.0):\n",
        "        \n",
        "        logits, state = self(next_token, context, state = state, return_state=True, training = False) \n",
        "\n",
        "        if temperature == 0.0:\n",
        "            next_token = tf.argmax(logits, axis=-1)\n",
        "        else:\n",
        "            logits = logits[:, -1, :]/temperature\n",
        "            next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "        # If a sequence produces an `end_token`, set it `done`\n",
        "        done = done | (next_token == self.end_token)\n",
        "        # Once a sequence is done it only produces 0-padding.\n",
        "        next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "        return next_token, done, state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cecfb5f9-66b3-49ea-8bd1-2d9f025a7d7e",
      "metadata": {
        "id": "cecfb5f9-66b3-49ea-8bd1-2d9f025a7d7e"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1b8acf32-bfc7-4e09-b669-bf6a1d7e1e36",
      "metadata": {
        "id": "1b8acf32-bfc7-4e09-b669-bf6a1d7e1e36"
      },
      "outputs": [],
      "source": [
        "class ChatBot(tf.keras.Model):\n",
        "    \n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self, text_processor, units, embed_dims):\n",
        "        super().__init__()\n",
        "        self.text_processor = text_processor\n",
        "        self.units = units\n",
        "        self.embed_dims = embed_dims\n",
        "        \n",
        "        # Build the encoder and decoder\n",
        "        encoder = Encoder(text_processor, units, embed_dims, embedding_matrix)\n",
        "        decoder = Decoder(text_processor, units, embed_dims, embedding_matrix)\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        context, x = inputs\n",
        "        context = self.encoder(context)\n",
        "        logits = self.decoder(x, context, training = True)\n",
        "\n",
        "        #TODO(b/250038731): remove this\n",
        "        try:\n",
        "          # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "            del logits._keras_mask\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "377ba89b-626c-48ed-922d-59579a3f4f8c",
      "metadata": {
        "id": "377ba89b-626c-48ed-922d-59579a3f4f8c"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "cea82227-e78f-426e-a082-5934bab185b2",
      "metadata": {
        "id": "cea82227-e78f-426e-a082-5934bab185b2"
      },
      "outputs": [],
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f541cf8-23a9-4dc7-b2a6-2a71293d7a20",
      "metadata": {
        "id": "5f541cf8-23a9-4dc7-b2a6-2a71293d7a20"
      },
      "source": [
        "# Compile and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4ea05d9d-b480-42b9-85bb-e7509eff5285",
      "metadata": {
        "id": "4ea05d9d-b480-42b9-85bb-e7509eff5285"
      },
      "outputs": [],
      "source": [
        "model = ChatBot(vectorizer, UNITS, EMBEDDING_DIMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7067ba74-31a8-4872-b12d-851b4474c2c2",
      "metadata": {
        "id": "7067ba74-31a8-4872-b12d-851b4474c2c2"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.005),\n",
        "              loss=masked_loss, \n",
        "              metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "e72f685c-28cb-4477-8aa7-6d45784a35f3",
      "metadata": {
        "id": "e72f685c-28cb-4477-8aa7-6d45784a35f3"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "CKPT_DIR = './model_checkpoint'\n",
        "# CKPT_DIR = '/content/drive/MyDrive/tf_model/chatbot'\n",
        "os.makedirs(CKPT_DIR, exist_ok = True)\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    os.path.join(CKPT_DIR,  f\"{datetime.now().strftime('%m:%d:%Y, %H:%M:%S')}\"),\n",
        "    monitor= 'masked_acc',\n",
        "    verbose= 0,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,\n",
        "    mode= 'auto',\n",
        "    save_freq='epoch'\n",
        ")\n",
        "\n",
        "os.makedirs('log', exist_ok = True)\n",
        "csv_logger = CSVLogger('./log/training.log')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.decoder.vocab_size"
      ],
      "metadata": {
        "id": "Isx924gHZgLz",
        "outputId": "e1257e96-bb96-4919-ded5-405ca479d404",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Isx924gHZgLz",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59905"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "bfcfa8af-e58d-41ed-896b-25686db9f034",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfcfa8af-e58d-41ed-896b-25686db9f034",
        "outputId": "3aa9aad3-fbc8-4125-bfca-7c2f6b3e4480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "50/50 [==============================] - 62s 1s/step - loss: 0.6787 - masked_acc: 0.8975 - masked_loss: 0.6787 - val_loss: 9.4493 - val_masked_acc: 0.0927 - val_masked_loss: 9.4493\n",
            "Epoch 2/30\n",
            "50/50 [==============================] - 53s 1s/step - loss: 0.5739 - masked_acc: 0.9085 - masked_loss: 0.5739 - val_loss: 9.6085 - val_masked_acc: 0.0986 - val_masked_loss: 9.6085\n",
            "Epoch 3/30\n",
            "50/50 [==============================] - 55s 1s/step - loss: 0.5436 - masked_acc: 0.9094 - masked_loss: 0.5436 - val_loss: 9.6451 - val_masked_acc: 0.0907 - val_masked_loss: 9.6451\n",
            "Epoch 4/30\n",
            "50/50 [==============================] - 53s 1s/step - loss: 0.4050 - masked_acc: 0.9294 - masked_loss: 0.4050 - val_loss: 10.0033 - val_masked_acc: 0.0978 - val_masked_loss: 10.0033\n",
            "Epoch 5/30\n",
            "50/50 [==============================] - 53s 1s/step - loss: 0.4253 - masked_acc: 0.9270 - masked_loss: 0.4253 - val_loss: 9.9797 - val_masked_acc: 0.0967 - val_masked_loss: 9.9797\n",
            "Epoch 6/30\n",
            "50/50 [==============================] - 53s 1s/step - loss: 0.3529 - masked_acc: 0.9424 - masked_loss: 0.3529 - val_loss: 9.9631 - val_masked_acc: 0.0904 - val_masked_loss: 9.9631\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "history = model.fit(\n",
        "    train_data.repeat(), \n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch = 50,\n",
        "    validation_data=test_data,\n",
        "    validation_steps = 2,\n",
        "    callbacks=[\n",
        "                tf.keras.callbacks.EarlyStopping(patience=5),\n",
        "                model_ckpt,\n",
        "                csv_logger]\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "qXjkrgPbK99z",
      "metadata": {
        "id": "qXjkrgPbK99z",
        "outputId": "0511e968-768f-4b3b-faef-23d4c11486f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_loss=0.352939248085022 \n",
            " model_accuracy=0.8974708318710327\n"
          ]
        }
      ],
      "source": [
        "model_loss = min(history.history['masked_loss'])\n",
        "model_accuracy = min(history.history['masked_acc']) \n",
        "print(f'{model_loss=} \\n {model_accuracy=}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "Vx5mxNIIkepN",
      "metadata": {
        "id": "Vx5mxNIIkepN"
      },
      "outputs": [],
      "source": [
        "# # saving model checkpoint to google drive\n",
        "\n",
        "# try:\n",
        "#     # from google.colab import drive\n",
        "#     # drive.mount('/content/drive')\n",
        "#     now = datetime.now().strftime('%m-%d-%Y_%H-%M-%S')\n",
        "#     print(now)\n",
        "#     folder_name = now+f'_epoch-{EPOCHS}'+f'_loss-{model_loss:.4f}'+f'_acc-{model_accuracy:.4f}'\n",
        "\n",
        "#     print(folder_name)\n",
        "#     ! cp model_checkpoint/ /content/drive/MyDrive/tf_model/$folder_name/ -r\n",
        "# except Exception as e:\n",
        "#     print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "xnKdNLeVHrn_",
      "metadata": {
        "id": "xnKdNLeVHrn_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "efa3096e-6f18-465a-ba4f-72cb3af03086",
      "metadata": {
        "id": "efa3096e-6f18-465a-ba4f-72cb3af03086"
      },
      "source": [
        "# Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "2ec71adf-c81f-45a2-a0e6-46cbe8cbfaff",
      "metadata": {
        "id": "2ec71adf-c81f-45a2-a0e6-46cbe8cbfaff"
      },
      "outputs": [],
      "source": [
        "@ChatBot.add_method\n",
        "def reply(self,\n",
        "              texts, *,\n",
        "              max_length=50,\n",
        "              temperature=0.0):\n",
        "    # Process the input texts\n",
        "    context = self.encoder.convert_input(texts, return_state = True)\n",
        "\n",
        "    context, state = context\n",
        "\n",
        "    batch_size = tf.shape(texts)[0]\n",
        "\n",
        "    # Setup the loop inputs\n",
        "    tokens = []\n",
        "    # attention_weights = []\n",
        "    next_token, done, state_zero = self.decoder.get_initial_state(context)\n",
        "    # state = state\n",
        "    state =[state_zero,state_zero]\n",
        "    for _ in range(max_length):\n",
        "        # Generate the next token\n",
        "        next_token, done, state = self.decoder.get_next_token(\n",
        "                next_token, context, done,  state, temperature)\n",
        "\n",
        "        # Collect the generated tokens\n",
        "        tokens.append(next_token)\n",
        "        # attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "            break\n",
        "\n",
        "    # Stack the lists of tokens and attention weights.\n",
        "    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "    # self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "    result = self.decoder.tokens_to_text(tokens)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "f7709f55-9f9f-4d37-b071-975678c5e045",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7709f55-9f9f-4d37-b071-975678c5e045",
        "outputId": "af91d41a-15e8-4b60-ff34-d09cccab5540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 : how was your date with john ? \n",
            "  1 : how was your date with john ? \n",
            "  2 : how was your date with john ? \n",
            "  3 : how was your date with john ? \n",
            "  4 : how was your date with john ? \n",
            "  5 : how was your date with john ? \n",
            "  6 : how was your date with john ? \n",
            "  7 : how was your date with john ? \n",
            "  8 : how was your date with john ? \n",
            "  9 : how was your date with stephen ? \n",
            " 10 : how was your date with stephen ? \n",
            " 11 : how was your date with john ? \n",
            " 12 : how was your date with stephen ? \n"
          ]
        }
      ],
      "source": [
        "for  i in range(0, 13):\n",
        "\n",
        "    result = model.reply(['how was your date with john ?'], temperature = i/10)\n",
        "    print(f'{i:3} : {result.numpy()[0].decode()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "c4edd67c-a52a-4dae-97ad-7492a234d26b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4edd67c-a52a-4dae-97ad-7492a234d26b",
        "outputId": "11ebc632-9f9b-4d94-d0dc-40f809a85050"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(221282, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('./dataset.csv', sep = '\\t', encoding='latin1', names = ['col1','col2']) \n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "3cc8e7f2-b6fe-4782-941f-dfffb8e060a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cc8e7f2-b6fe-4782-941f-dfffb8e060a0",
        "outputId": "2a57c49b-a09b-4cbd-c315-5551b92bab39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input     :  I am a fool.\n",
            "Output    :  That depends upon your motive. You were in love with Mitchell, weren't you?\n",
            "Predicted :  i am a fool . \n",
            "\n",
            "Input     :  I'm going... I am... I'm going to be sick.\n",
            "Output    :  No you're not. How... Nice one. How dumb do you think I am ?\n",
            "Predicted :  im going . . . i am . . . im going to be sick . \n",
            "\n",
            "Input     :  He sells insurance for Metropolitan Life.\n",
            "Output    :  Be tough on 'em.\n",
            "Predicted :  he outlets insurance for state carolina life . \n",
            "\n",
            "Input     :  That cop!  Why'd you have to do that?  You couldn't wound him?  The guy had a family, maybe, parents, kids who gotta grow up without a dad, he was probably a good guy; and he believed me...\n",
            "Output    :  I shoulda saved him 'cause he <u>believed</u> you?\n",
            "Predicted :  that cop ! whyd you have to do that ? you couldnt lick him ? the guy had a family , maybe , kids had a family , maybe , kids who gotta grow up without a dad , he believed me . . . \n",
            "\n",
            "Input     :  You're supposed to film it, not turn it to hash!\n",
            "Output    :  Look, Doctor, this isn't one of your pet porpoises we're after. It's a torpedo of muscle. Murderous expertise!  I've seen bleeding sharks try to eat themselves!  Don't be so concerned about them.  Worry about yourself.\n",
            "Predicted :  youre supposed to film it , not turn it to hoof ! \n",
            "\n",
            "Input     :  I... I don't follow.\n",
            "Output    :  All this effort you've made to get transferred, it's the first question that pops into my head.\n",
            "Predicted :  i . . . i dont follow . \n",
            "\n",
            "Input     :  What is it with you and the space program?\n",
            "Output    :  And look here.  Cease fire in Chechenia.  That's good for the banks who lent the government money, but bad for the guys selling them weapons.  Listen to this, some gas company in Colorado.  Their researchers have been blocked from testing a fuel additive.  They've accused the E.P.A. of, quote, 'turning a blind <u>eye</u> to the future.'\n",
            "Predicted :  what is it with you and the space program ? \n",
            "\n",
            "Input     :  Good evening -- I'm sorry, Jabez -- I'm a little late.\n",
            "Output    :  No, you're not.\n",
            "Predicted :  good morning im sorry , schultz im a little late . \n",
            "\n",
            "Input     :  Really?\n",
            "Output    :  So where is young Jonah?\n",
            "Predicted :  really ? \n",
            "\n",
            "Input     :  If you don't want it, give it back.\n",
            "Output    :  I don't \"want\" it, if you catch my drift.\n",
            "Predicted :  if you dont want it , give it back . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_num = 10\n",
        "for i in range(test_num):\n",
        "    a = data.sample(1)\n",
        "    print('Input     : ',a['col1'].values[0])\n",
        "    print('Output    : ',a['col2'].values[0])\n",
        "    result = model.reply(a['col1'].values, temperature = .6)\n",
        "    print('Predicted : ',    result.numpy()[0].decode())\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2c7934f0-9567-4cc8-a07a-0c956220e124",
      "metadata": {
        "id": "2c7934f0-9567-4cc8-a07a-0c956220e124"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "13c09f75-88d5-4dc4-9a42-398b7a17a4d0",
      "metadata": {
        "id": "13c09f75-88d5-4dc4-9a42-398b7a17a4d0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2b586d8e-0b41-4f2c-b20a-f6e05aa051dc",
      "metadata": {
        "id": "2b586d8e-0b41-4f2c-b20a-f6e05aa051dc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b7447dbf-c935-4080-8230-c827a38e98d1",
      "metadata": {
        "id": "b7447dbf-c935-4080-8230-c827a38e98d1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d52f1642-700a-42ec-900f-41c19dc9cf97",
      "metadata": {
        "id": "d52f1642-700a-42ec-900f-41c19dc9cf97"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3f40775f-25aa-45e9-9b2c-5ef2a47bafee",
      "metadata": {
        "id": "3f40775f-25aa-45e9-9b2c-5ef2a47bafee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d8add436-3533-41ca-b864-63a85f5a30c2",
      "metadata": {
        "id": "d8add436-3533-41ca-b864-63a85f5a30c2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4158bb21-094d-4bd9-bf1f-8a209d980676",
      "metadata": {
        "id": "4158bb21-094d-4bd9-bf1f-8a209d980676"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "de4b7996-8c34-4c51-a707-60caf8921a84",
      "metadata": {
        "id": "de4b7996-8c34-4c51-a707-60caf8921a84"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f366abef-cd80-42bb-b680-ad634d234eb9",
      "metadata": {
        "id": "f366abef-cd80-42bb-b680-ad634d234eb9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "38d8fb6b-0362-4a39-b06b-af08bee0cf6d",
      "metadata": {
        "id": "38d8fb6b-0362-4a39-b06b-af08bee0cf6d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "13198b71-3dcf-4218-94b1-994db06e7b89",
      "metadata": {
        "id": "13198b71-3dcf-4218-94b1-994db06e7b89"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f98e0ed1-7a81-4793-91a1-6839b9864bd8",
      "metadata": {
        "id": "f98e0ed1-7a81-4793-91a1-6839b9864bd8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "61097607-493e-4ada-979b-492f874d0ded",
      "metadata": {
        "id": "61097607-493e-4ada-979b-492f874d0ded"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}