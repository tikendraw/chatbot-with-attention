{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tikendraw/chatbot-with-attention/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatBot Training\n",
        "\n"
      ],
      "metadata": {
        "id": "581c485f-6b99-4710-a4db-9b7a5b1953be"
      },
      "id": "581c485f-6b99-4710-a4db-9b7a5b1953be"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6oigFEoxv3qn",
        "outputId": "b865f3ec-de8c-434c-d2c7-466709f9fd74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6oigFEoxv3qn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "de9895a2-8aa5-4a97-8c09-537f95dc8648",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de9895a2-8aa5-4a97-8c09-537f95dc8648",
        "outputId": "326b1b2a-3ede-400b-9661-bea280ff6e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chatbot-with-attention'...\n",
            "remote: Enumerating objects: 261, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 261 (delta 28), reused 65 (delta 17), pack-reused 176\u001b[K\n",
            "Receiving objects: 100% (261/261), 239.73 MiB | 19.19 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "Updating files: 100% (29/29), done.\n",
            "/content/chatbot-with-attention\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    \n",
        "    # Mount Google drive\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    \n",
        "    ! git clone https://github.com/tikendraw/chatbot-with-attention.git \n",
        "    os.chdir('chatbot-with-attention') \n",
        "    print(os.getcwd())\n",
        "\n",
        "    ! pip install tensorflow==2.11 -q\n",
        "    ! pip install tensorflow-text -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f021dd31-9ec0-4039-bbe7-10271ec9640a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f021dd31-9ec0-4039-bbe7-10271ec9640a",
        "outputId": "a1bfc604-cfe4-484f-8a9a-bfd24286244a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Avaliable:  1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    TextVectorization, \n",
        "    Embedding, \n",
        "    LSTM, \n",
        "    GRU, \n",
        "    Bidirectional, \n",
        "    Dense, \n",
        "    Attention, \n",
        "    Concatenate\n",
        ")\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow_text as tf_text\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "print('GPU Avaliable: ', gpu:=len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d3bc845-ba26-4330-9ed5-cda72941e51c",
      "metadata": {
        "id": "3d3bc845-ba26-4330-9ed5-cda72941e51c"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "61c8b316-519e-4baf-931d-14fe9ce2dc58",
      "metadata": {
        "id": "61c8b316-519e-4baf-931d-14fe9ce2dc58"
      },
      "outputs": [],
      "source": [
        "MAX_OUTPUT_LENGTH = 200\n",
        "BATCH_SIZE = 32\n",
        "UNITS = 64\n",
        "EMBEDDING_DIMS = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3806af9-4f24-42e4-ba56-20c4e3f386aa",
      "metadata": {
        "id": "e3806af9-4f24-42e4-ba56-20c4e3f386aa"
      },
      "source": [
        "# Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cb91ce87-6d26-48a6-adc7-c83c584495f5",
      "metadata": {
        "id": "cb91ce87-6d26-48a6-adc7-c83c584495f5"
      },
      "outputs": [],
      "source": [
        "# preprocessing text\n",
        "def tf_lower_and_split_punct_en(text):\n",
        "    # Split accented characters.\n",
        "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "    text = tf.strings.lower(text)\n",
        "    # Keep space, a to z, and select punctuation.\n",
        "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "    # Add spaces around punctuation.\n",
        "    text = tf.strings.regex_replace(text, '[.?!,¿|]', r' \\0 ')\n",
        "    # Strip whitespace.\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a58406b5-b0ae-4c65-952a-d43e27e34c5a",
      "metadata": {
        "id": "a58406b5-b0ae-4c65-952a-d43e27e34c5a"
      },
      "outputs": [],
      "source": [
        "# Loading vectorizer\n",
        "from_disk = pickle.load(open(\"./components/vectorizer.pkl\", \"rb\"))\n",
        "vectorizer = TextVectorization.from_config(from_disk['config'])\n",
        "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "vectorizer.set_weights(from_disk['weights'])\n",
        "\n",
        "# Lets see the Vector for word \"this\"\n",
        "# print (vectorizer(\"who am i\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217530b2-25b8-4622-a415-e36f31306d1a",
      "metadata": {
        "id": "217530b2-25b8-4622-a415-e36f31306d1a"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0e1144e5-e080-4366-88f6-bd52f5b0b154",
      "metadata": {
        "id": "0e1144e5-e080-4366-88f6-bd52f5b0b154"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "32CCNnGzXTmF",
        "outputId": "60ec7cbb-81b5-4db8-dec5-d81e5df750a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "32CCNnGzXTmF",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " components\t\t\t    LICENSE\n",
            "'cornell movie-dialogs corpus'\t    movie_dialogue_chatbot.ipynb\n",
            " cornell_movie_dialogs_corpus.zip   preprocessing_cornelldata.py\n",
            " dataset\t\t\t    preprocess.py\n",
            " dataset.csv\t\t\t    README.md\n",
            " embedding\t\t\t    TextFile.txt\n",
            " funcyou\t\t\t    training.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "zipf = zipfile.ZipFile('./embedding/embedding_matrix.zip')\n",
        "zipf.extractall('./embedding/')\n",
        "zipf.close()"
      ],
      "metadata": {
        "id": "1OCHcqCdXdOU"
      },
      "id": "1OCHcqCdXdOU",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f8fbbc0a-dfaf-4b25-84aa-7592ab122c6d",
      "metadata": {
        "id": "f8fbbc0a-dfaf-4b25-84aa-7592ab122c6d"
      },
      "outputs": [],
      "source": [
        "# Loading embedding_matrix\n",
        "embedding_matrix = np.load('./embedding/embedding_matrix.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1a98d151-06af-416d-a9ca-33a1ad9ce42b",
      "metadata": {
        "id": "1a98d151-06af-416d-a9ca-33a1ad9ce42b",
        "outputId": "33c03c0d-7d44-4a8d-829a-2ba7db0d2262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59905, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "da7b08f1-373d-4866-b2ae-7ce56f43fb77",
      "metadata": {
        "id": "da7b08f1-373d-4866-b2ae-7ce56f43fb77"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dbb0dc2a-07fc-4094-b47f-4e82316806b6",
      "metadata": {
        "id": "dbb0dc2a-07fc-4094-b47f-4e82316806b6"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a260a269-ff87-4944-9573-1ccad9210800",
      "metadata": {
        "id": "a260a269-ff87-4944-9573-1ccad9210800"
      },
      "outputs": [],
      "source": [
        "save_train_data_path = './dataset/train/'\n",
        "save_test_data_path = './dataset/test/'\n",
        "\n",
        "#loading the data\n",
        "train_data = tf.data.Dataset.load(save_train_data_path, compression='GZIP')\n",
        "test_data = tf.data.Dataset.load(save_test_data_path, compression='GZIP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a06b2375-303c-4c5a-8e3a-2019782cdfd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a06b2375-303c-4c5a-8e3a-2019782cdfd2",
        "outputId": "3a0172c6-576d-4c36-c48a-6e653ca2260b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder input\n",
            "[    3    58    33   115    22   993     7 48680 52632    13  4132  8573\n",
            "    40   421    82  3902 24129   978   516    54]\n",
            "--------------------------------------------\n",
            "decoder input\n",
            "[    3    52     5     8   150   639   328    39 34888     5    48    57\n",
            "   119    39     6     2     4     0     0     0]\n",
            "--------------------------------------------\n",
            "encoder output\n",
            "[   52     5     8   150   639   328    39 34888     5    48    57   119\n",
            "    39     6     2     4     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "for (enc_input, dec_input), dec_output  in train_data.take(1):\n",
        "    print('encoder input')\n",
        "    print(enc_input[0, :20].numpy())\n",
        "    print('-'*44)\n",
        "    print('decoder input')\n",
        "    print(dec_input[0, :20].numpy()) \n",
        "    print('-'*44)\n",
        "    print('encoder output')\n",
        "    print(dec_output[0, :20].numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acf3eb4b-2d2d-4df3-96a5-2a496bfefa63",
      "metadata": {
        "id": "acf3eb4b-2d2d-4df3-96a5-2a496bfefa63"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "86c16205-0b6a-4e4a-8551-5fe31ed729da",
      "metadata": {
        "id": "86c16205-0b6a-4e4a-8551-5fe31ed729da"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c6eeb3-3f2b-4292-a206-a7abd0c07218",
      "metadata": {
        "id": "43c6eeb3-3f2b-4292-a206-a7abd0c07218"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "af526d19-6fc6-48fe-8470-4789bc715701",
      "metadata": {
        "id": "af526d19-6fc6-48fe-8470-4789bc715701"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, text_vectorizer, units, embed_dims, embedding_matrix=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.text_vectorizer =  text_vectorizer\n",
        "        self.units = units\n",
        "        self.embed_dims = embed_dims\n",
        "        self.vocab_size = text_vectorizer.vocabulary_size()\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size , output_dim=self.embed_dims, mask_zero=True, trainable=False)\n",
        "        self.embedding.build((None,))\n",
        "        self.embedding.set_weights([embedding_matrix])\n",
        "        self.rnn = Bidirectional(merge_mode='concat', layer = LSTM(self.units, return_sequences=True, return_state=True))\n",
        "        \n",
        "    def call(self, x, y=None, return_state=False):\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        encoder_output, forward_h, forward_c, backward_h, backward_c, = self.rnn(x)\n",
        "        \n",
        "        state_h = Concatenate()([forward_h, backward_h])\n",
        "        state_c = Concatenate()([forward_c, backward_c])\n",
        "        \n",
        "        encoder_state = [state_h, state_c]\n",
        "        \n",
        "        if return_state:\n",
        "            return encoder_output, encoder_state\n",
        "        else:\n",
        "            return encoder_output\n",
        "        \n",
        "    def convert_input(self, texts, return_state=False):\n",
        "        texts = tf.convert_to_tensor(texts)\n",
        "        if len(texts.shape) == 0:\n",
        "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "        context = self.text_vectorizer(texts)\n",
        "        \n",
        "        context = self(context, return_state = return_state)\n",
        "        \n",
        "        return context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242c26be-d2b5-4d45-90d0-cad07e0d6496",
      "metadata": {
        "id": "242c26be-d2b5-4d45-90d0-cad07e0d6496"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a39d0588-f0e0-4fe3-bdca-d24cc834b802",
      "metadata": {
        "id": "a39d0588-f0e0-4fe3-bdca-d24cc834b802"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "    def __init__(self, text_vectorizer, units,  embed_dims, embedding_matrix=None) :\n",
        "        super(Decoder, self).__init__()\n",
        "        self.text_vectorizer =  text_vectorizer\n",
        "        self.units = units * 2\n",
        "        self.embed_dims = embed_dims\n",
        "        self.vocab_size = text_vectorizer.vocabulary_size()\n",
        "        \n",
        "        self.embedding = Embedding(input_dim=self.vocab_size , output_dim=self.embed_dims, mask_zero=True, trainable=False)\n",
        "        self.embedding.build((None,))\n",
        "        self.embedding.set_weights([embedding_matrix])\n",
        "        \n",
        "        self.rnn = LSTM(self.units, return_sequences=True, return_state=True)\n",
        "        \n",
        "        # self.attention =  tf.keras.layers.Attention()\n",
        "        self.attention = AttentionLayer()\n",
        "        \n",
        "        self.output_dense = Dense(self.vocab_size)\n",
        "        \n",
        "        self.word_to_id = tf.keras.layers.StringLookup(vocabulary=text_vectorizer.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "        self.id_to_word = tf.keras.layers.StringLookup(vocabulary=text_vectorizer.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
        "        \n",
        "        self.start_token = self.word_to_id('[START]')\n",
        "        self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    def call(self, x, context, state=None, return_state = False, training=False):\n",
        "        ''' x, context, state=None, return_sequence=False '''\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        decoder_output, decoder_state_h, decoder_state_c = self.rnn(x, initial_state=state)\n",
        "        decoder_state = [decoder_state_h, decoder_state_c]\n",
        "        \n",
        "        # attention\n",
        "        # attn_op= self.attention([context, decoder_output])\n",
        "        attn_op, attn_state = self.attention([context, decoder_output]) # this is for custom AttentionLayer()\n",
        "\n",
        "        x = Concatenate(axis=-1)([decoder_output, attn_op])\n",
        "        # x = tf.multiply(decoder_output, attn_op)\n",
        "        # x = self.attention(decoder_output, context)\n",
        "        # self.last_attention_weights = self.attention.last_attention_weights\n",
        "\n",
        "        logits = self.output_dense(x)\n",
        "            \n",
        "        if return_state:\n",
        "            return logits, decoder_state\n",
        "        else:\n",
        "            return logits\n",
        "        \n",
        "    def get_initial_state(self, context):\n",
        "        batch_size = tf.shape(context)[0]\n",
        "        start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "        embedded = self.embedding(start_tokens)\n",
        "        return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
        "\n",
        "    \n",
        "    def tokens_to_text(self, tokens):\n",
        "        words = self.id_to_word(tokens)\n",
        "        result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "        result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "        result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "        return result\n",
        "    \n",
        "    def get_next_token(self, next_token, context,  done, state, temperature = 0.0):\n",
        "        \n",
        "        logits, state = self(next_token, context, state = state, return_state=True, training = False) \n",
        "\n",
        "        if temperature == 0.0:\n",
        "            next_token = tf.argmax(logits, axis=-1)\n",
        "        else:\n",
        "            logits = logits[:, -1, :]/temperature\n",
        "            next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "        # If a sequence produces an `end_token`, set it `done`\n",
        "        done = done | (next_token == self.end_token)\n",
        "        # Once a sequence is done it only produces 0-padding.\n",
        "        next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "        return next_token, done, state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the encoder and decoder\n",
        "encoder = Encoder(vectorizer, UNITS, EMBEDDING_DIMS, embedding_matrix)\n",
        "decoder = Decoder(vectorizer, UNITS, EMBEDDING_DIMS, embedding_matrix)\n",
        "        \n",
        "context, x = enc_input, dec_input\n",
        "context = encoder(context)\n",
        "logits = decoder(x, context, training = False)\n",
        "\n",
        "print(len(logits))\n",
        "\n",
        "# decoder_output, att_op = logits[1]\n",
        "# print(decoder_output.shape, att_op.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkWEHZ7Grmp1",
        "outputId": "21e439d4-422c-42c7-e97f-1b95bfbfe623"
      },
      "id": "ZkWEHZ7Grmp1",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context.shape, logits.shape"
      ],
      "metadata": {
        "id": "pnbUc09jyzfG",
        "outputId": "f32c867a-7471-4661-deb1-be3f39ced72d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pnbUc09jyzfG",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 199, 128]), TensorShape([32, 199, 59905]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize = (30, 10))\n",
        "# p1 = decoder_output[0,:,:]\n",
        "# p2 = att_op[0,:,:]\n",
        "\n",
        "# plt.subplot(1,3,1)\n",
        "# sns.heatmap(p1)\n",
        "\n",
        "\n",
        "# plt.subplot(1,3,2)\n",
        "# sns.heatmap(p2)\n",
        "\n",
        "\n",
        "# plt.subplot(1,3,3)\n",
        "# sns.heatmap(tf.multiply(p2,p1))"
      ],
      "metadata": {
        "id": "UcoMd-9NstMc"
      },
      "id": "UcoMd-9NstMc",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cecfb5f9-66b3-49ea-8bd1-2d9f025a7d7e",
      "metadata": {
        "id": "cecfb5f9-66b3-49ea-8bd1-2d9f025a7d7e"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1b8acf32-bfc7-4e09-b669-bf6a1d7e1e36",
      "metadata": {
        "id": "1b8acf32-bfc7-4e09-b669-bf6a1d7e1e36"
      },
      "outputs": [],
      "source": [
        "class ChatBot(tf.keras.Model):\n",
        "    \n",
        "    @classmethod\n",
        "    def add_method(cls, fun):\n",
        "        setattr(cls, fun.__name__, fun)\n",
        "        return fun\n",
        "\n",
        "    def __init__(self, text_processor, units, embed_dims):\n",
        "        super().__init__()\n",
        "        self.text_processor = text_processor\n",
        "        self.units = units\n",
        "        self.embed_dims = embed_dims\n",
        "        \n",
        "        # Build the encoder and decoder\n",
        "        encoder = Encoder(text_processor, units, embed_dims, embedding_matrix)\n",
        "        decoder = Decoder(text_processor, units, embed_dims, embedding_matrix)\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        context, x = inputs\n",
        "        context = self.encoder(context)\n",
        "        logits = self.decoder(x, context, training = True)\n",
        "\n",
        "        #TODO(b/250038731): remove this\n",
        "        try:\n",
        "          # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "            del logits._keras_mask\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "377ba89b-626c-48ed-922d-59579a3f4f8c",
      "metadata": {
        "id": "377ba89b-626c-48ed-922d-59579a3f4f8c"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cea82227-e78f-426e-a082-5934bab185b2",
      "metadata": {
        "id": "cea82227-e78f-426e-a082-5934bab185b2"
      },
      "outputs": [],
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f541cf8-23a9-4dc7-b2a6-2a71293d7a20",
      "metadata": {
        "id": "5f541cf8-23a9-4dc7-b2a6-2a71293d7a20"
      },
      "source": [
        "# Compile and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4ea05d9d-b480-42b9-85bb-e7509eff5285",
      "metadata": {
        "id": "4ea05d9d-b480-42b9-85bb-e7509eff5285"
      },
      "outputs": [],
      "source": [
        "model = ChatBot(vectorizer, UNITS, EMBEDDING_DIMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "7067ba74-31a8-4872-b12d-851b4474c2c2",
      "metadata": {
        "id": "7067ba74-31a8-4872-b12d-851b4474c2c2"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.005),\n",
        "              loss=masked_loss, \n",
        "              metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e72f685c-28cb-4477-8aa7-6d45784a35f3",
      "metadata": {
        "id": "e72f685c-28cb-4477-8aa7-6d45784a35f3"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 40\n",
        "\n",
        "CKPT_DIR = './model_checkpoint'\n",
        "# CKPT_DIR = '/content/drive/MyDrive/tf_model/chatbot'\n",
        "os.makedirs(CKPT_DIR, exist_ok = True)\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    os.path.join(CKPT_DIR,  f\"{datetime.now().strftime('%m:%d:%Y, %H:%M:%S')}\"),\n",
        "    monitor= 'masked_acc',\n",
        "    verbose= 0,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,\n",
        "    mode= 'auto',\n",
        "    save_freq='epoch'\n",
        ")\n",
        "\n",
        "os.makedirs('log', exist_ok = True)\n",
        "csv_logger = CSVLogger('./log/training.log')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Isx924gHZgLz"
      },
      "id": "Isx924gHZgLz",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "bfcfa8af-e58d-41ed-896b-25686db9f034",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfcfa8af-e58d-41ed-896b-25686db9f034",
        "outputId": "8e7d4f8c-13d3-4f5c-ac3e-e5a555be9222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50/50 [==============================] - 102s 1s/step - loss: 7.0483 - masked_acc: 0.1044 - masked_loss: 7.0483 - val_loss: 6.3622 - val_masked_acc: 0.1353 - val_masked_loss: 6.3622\n",
            "Epoch 2/40\n",
            "50/50 [==============================] - 59s 1s/step - loss: 6.0048 - masked_acc: 0.1344 - masked_loss: 6.0048 - val_loss: 6.0432 - val_masked_acc: 0.1341 - val_masked_loss: 6.0432\n",
            "Epoch 3/40\n",
            "50/50 [==============================] - 59s 1s/step - loss: 6.0424 - masked_acc: 0.1218 - masked_loss: 6.0424 - val_loss: 5.9980 - val_masked_acc: 0.1017 - val_masked_loss: 5.9980\n",
            "Epoch 4/40\n",
            "50/50 [==============================] - 61s 1s/step - loss: 5.7792 - masked_acc: 0.1473 - masked_loss: 5.7792 - val_loss: 5.8155 - val_masked_acc: 0.1817 - val_masked_loss: 5.8155\n",
            "Epoch 5/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.7840 - masked_acc: 0.1820 - masked_loss: 5.7840 - val_loss: 5.6464 - val_masked_acc: 0.2001 - val_masked_loss: 5.6464\n",
            "Epoch 6/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.6623 - masked_acc: 0.1754 - masked_loss: 5.6623 - val_loss: 5.5688 - val_masked_acc: 0.2061 - val_masked_loss: 5.5688\n",
            "Epoch 7/40\n",
            "50/50 [==============================] - 59s 1s/step - loss: 5.4807 - masked_acc: 0.1723 - masked_loss: 5.4807 - val_loss: 5.5286 - val_masked_acc: 0.2037 - val_masked_loss: 5.5286\n",
            "Epoch 8/40\n",
            "50/50 [==============================] - 61s 1s/step - loss: 5.2041 - masked_acc: 0.2111 - masked_loss: 5.2041 - val_loss: 5.4880 - val_masked_acc: 0.2099 - val_masked_loss: 5.4880\n",
            "Epoch 9/40\n",
            "50/50 [==============================] - 59s 1s/step - loss: 5.2702 - masked_acc: 0.2095 - masked_loss: 5.2702 - val_loss: 5.4052 - val_masked_acc: 0.2174 - val_masked_loss: 5.4052\n",
            "Epoch 10/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.3704 - masked_acc: 0.1914 - masked_loss: 5.3704 - val_loss: 5.3472 - val_masked_acc: 0.2175 - val_masked_loss: 5.3472\n",
            "Epoch 11/40\n",
            "50/50 [==============================] - 62s 1s/step - loss: 5.3810 - masked_acc: 0.2008 - masked_loss: 5.3810 - val_loss: 5.3267 - val_masked_acc: 0.2256 - val_masked_loss: 5.3267\n",
            "Epoch 12/40\n",
            "50/50 [==============================] - 59s 1s/step - loss: 5.2009 - masked_acc: 0.2031 - masked_loss: 5.2009 - val_loss: 5.3320 - val_masked_acc: 0.2175 - val_masked_loss: 5.3320\n",
            "Epoch 13/40\n",
            "50/50 [==============================] - 62s 1s/step - loss: 5.2015 - masked_acc: 0.2141 - masked_loss: 5.2015 - val_loss: 5.2719 - val_masked_acc: 0.2207 - val_masked_loss: 5.2719\n",
            "Epoch 14/40\n",
            "50/50 [==============================] - 61s 1s/step - loss: 5.1318 - masked_acc: 0.2035 - masked_loss: 5.1318 - val_loss: 5.2310 - val_masked_acc: 0.2295 - val_masked_loss: 5.2310\n",
            "Epoch 15/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.3810 - masked_acc: 0.1924 - masked_loss: 5.3810 - val_loss: 5.2568 - val_masked_acc: 0.2245 - val_masked_loss: 5.2568\n",
            "Epoch 16/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.1918 - masked_acc: 0.2017 - masked_loss: 5.1918 - val_loss: 5.2181 - val_masked_acc: 0.2219 - val_masked_loss: 5.2181\n",
            "Epoch 17/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.1329 - masked_acc: 0.2065 - masked_loss: 5.1329 - val_loss: 5.2306 - val_masked_acc: 0.2319 - val_masked_loss: 5.2306\n",
            "Epoch 18/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 4.8463 - masked_acc: 0.2215 - masked_loss: 4.8463 - val_loss: 5.1919 - val_masked_acc: 0.2226 - val_masked_loss: 5.1919\n",
            "Epoch 19/40\n",
            "50/50 [==============================] - 62s 1s/step - loss: 5.3232 - masked_acc: 0.2101 - masked_loss: 5.3232 - val_loss: 5.1169 - val_masked_acc: 0.2290 - val_masked_loss: 5.1169\n",
            "Epoch 20/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 4.8637 - masked_acc: 0.2445 - masked_loss: 4.8637 - val_loss: 5.0905 - val_masked_acc: 0.2367 - val_masked_loss: 5.0905\n",
            "Epoch 21/40\n",
            "50/50 [==============================] - 62s 1s/step - loss: 5.1383 - masked_acc: 0.2020 - masked_loss: 5.1383 - val_loss: 5.0566 - val_masked_acc: 0.2234 - val_masked_loss: 5.0566\n",
            "Epoch 22/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 4.9828 - masked_acc: 0.2274 - masked_loss: 4.9828 - val_loss: 5.0405 - val_masked_acc: 0.2253 - val_masked_loss: 5.0405\n",
            "Epoch 23/40\n",
            "50/50 [==============================] - 61s 1s/step - loss: 4.7259 - masked_acc: 0.2568 - masked_loss: 4.7259 - val_loss: 5.0636 - val_masked_acc: 0.2273 - val_masked_loss: 5.0636\n",
            "Epoch 24/40\n",
            "50/50 [==============================] - 58s 1s/step - loss: 5.0951 - masked_acc: 0.2178 - masked_loss: 5.0951 - val_loss: 4.9918 - val_masked_acc: 0.2437 - val_masked_loss: 4.9918\n",
            "Epoch 25/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.1853 - masked_acc: 0.1955 - masked_loss: 5.1853 - val_loss: 4.9878 - val_masked_acc: 0.2389 - val_masked_loss: 4.9878\n",
            "Epoch 26/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 4.9448 - masked_acc: 0.2298 - masked_loss: 4.9448 - val_loss: 4.9614 - val_masked_acc: 0.2425 - val_masked_loss: 4.9614\n",
            "Epoch 27/40\n",
            "50/50 [==============================] - 59s 1s/step - loss: 4.8823 - masked_acc: 0.2395 - masked_loss: 4.8823 - val_loss: 4.9659 - val_masked_acc: 0.2438 - val_masked_loss: 4.9659\n",
            "Epoch 28/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.0856 - masked_acc: 0.2291 - masked_loss: 5.0856 - val_loss: 4.9838 - val_masked_acc: 0.2369 - val_masked_loss: 4.9838\n",
            "Epoch 29/40\n",
            "50/50 [==============================] - 59s 1s/step - loss: 4.7835 - masked_acc: 0.2340 - masked_loss: 4.7835 - val_loss: 4.9526 - val_masked_acc: 0.2488 - val_masked_loss: 4.9526\n",
            "Epoch 30/40\n",
            "50/50 [==============================] - 62s 1s/step - loss: 4.8292 - masked_acc: 0.2328 - masked_loss: 4.8292 - val_loss: 4.9404 - val_masked_acc: 0.2379 - val_masked_loss: 4.9404\n",
            "Epoch 31/40\n",
            "50/50 [==============================] - 59s 1s/step - loss: 4.8777 - masked_acc: 0.2408 - masked_loss: 4.8777 - val_loss: 4.9097 - val_masked_acc: 0.2491 - val_masked_loss: 4.9097\n",
            "Epoch 32/40\n",
            "50/50 [==============================] - 58s 1s/step - loss: 4.9857 - masked_acc: 0.2189 - masked_loss: 4.9857 - val_loss: 4.8958 - val_masked_acc: 0.2459 - val_masked_loss: 4.8958\n",
            "Epoch 33/40\n",
            "50/50 [==============================] - 61s 1s/step - loss: 5.4637 - masked_acc: 0.1944 - masked_loss: 5.4637 - val_loss: 4.9286 - val_masked_acc: 0.2336 - val_masked_loss: 4.9286\n",
            "Epoch 34/40\n",
            "50/50 [==============================] - 59s 1s/step - loss: 4.7481 - masked_acc: 0.2359 - masked_loss: 4.7481 - val_loss: 4.8822 - val_masked_acc: 0.2491 - val_masked_loss: 4.8822\n",
            "Epoch 35/40\n",
            "50/50 [==============================] - 61s 1s/step - loss: 4.9600 - masked_acc: 0.2316 - masked_loss: 4.9600 - val_loss: 4.8779 - val_masked_acc: 0.2516 - val_masked_loss: 4.8779\n",
            "Epoch 36/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.0373 - masked_acc: 0.2209 - masked_loss: 5.0373 - val_loss: 4.8577 - val_masked_acc: 0.2534 - val_masked_loss: 4.8577\n",
            "Epoch 37/40\n",
            "50/50 [==============================] - 61s 1s/step - loss: 4.8146 - masked_acc: 0.2272 - masked_loss: 4.8146 - val_loss: 4.8458 - val_masked_acc: 0.2534 - val_masked_loss: 4.8458\n",
            "Epoch 38/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.0506 - masked_acc: 0.2220 - masked_loss: 5.0506 - val_loss: 4.8107 - val_masked_acc: 0.2422 - val_masked_loss: 4.8107\n",
            "Epoch 39/40\n",
            "50/50 [==============================] - 62s 1s/step - loss: 4.4901 - masked_acc: 0.2566 - masked_loss: 4.4901 - val_loss: 4.8464 - val_masked_acc: 0.2438 - val_masked_loss: 4.8464\n",
            "Epoch 40/40\n",
            "50/50 [==============================] - 60s 1s/step - loss: 5.1183 - masked_acc: 0.2043 - masked_loss: 5.1183 - val_loss: 4.8290 - val_masked_acc: 0.2547 - val_masked_loss: 4.8290\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "history = model.fit(\n",
        "    train_data.repeat(), \n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch = 50,\n",
        "    validation_data=test_data,\n",
        "    validation_steps = 2,\n",
        "    callbacks=[\n",
        "                # tf.keras.callbacks.EarlyStopping(patience=5),\n",
        "                model_ckpt,\n",
        "                csv_logger]\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading model checkpoint\n",
        "# MODEL_CHECKPOINT_DIR = '/content/drive/MyDrive/tf_model/03-01-2023_07-07-48_epoch-30_loss-0.4649_acc-0.3174'\n",
        "\n",
        "# model.load_weights(MODEL_CHECKPOINT_DIR)\n"
      ],
      "metadata": {
        "id": "NOIQfXUAxWbZ"
      },
      "id": "NOIQfXUAxWbZ",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "qXjkrgPbK99z",
      "metadata": {
        "id": "qXjkrgPbK99z",
        "outputId": "8adbefb6-3de3-4811-f4c6-9210f3570190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_loss=4.490143299102783 \n",
            " model_accuracy=0.10441351681947708\n"
          ]
        }
      ],
      "source": [
        "model_loss = min(history.history['masked_loss'])\n",
        "model_accuracy = min(history.history['masked_acc']) \n",
        "print(f'{model_loss=} \\n {model_accuracy=}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # saving a model as h5\n",
        "\n",
        "# now = datetime.now().strftime('%m-%d-%Y_%H-%M-%S')\n",
        "# model_name = 'chatbot_with_attention-'+now+f'_epoch-{EPOCHS}'+f'_loss-{model_loss:.4f}'+f'_acc-{model_accuracy:.4f}'\n",
        "\n",
        "# model.save_weights(f'./saved_model/{model_name}')\n",
        "\n",
        "# model.save(f'./saved_model/{model_name}_full', save_format='tf')\n"
      ],
      "metadata": {
        "id": "99vWQ55dDlqA"
      },
      "id": "99vWQ55dDlqA",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "efa3096e-6f18-465a-ba4f-72cb3af03086",
      "metadata": {
        "id": "efa3096e-6f18-465a-ba4f-72cb3af03086"
      },
      "source": [
        "# Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2ec71adf-c81f-45a2-a0e6-46cbe8cbfaff",
      "metadata": {
        "id": "2ec71adf-c81f-45a2-a0e6-46cbe8cbfaff"
      },
      "outputs": [],
      "source": [
        "@ChatBot.add_method\n",
        "def reply(self,\n",
        "              texts, *,\n",
        "              max_length=50,\n",
        "              temperature=0.0):\n",
        "    # Process the input texts\n",
        "    context = self.encoder.convert_input(texts, return_state = True)\n",
        "\n",
        "    context, state = context\n",
        "\n",
        "    batch_size = tf.shape(texts)[0]\n",
        "\n",
        "    # Setup the loop inputs\n",
        "    tokens = []\n",
        "    # attention_weights = []\n",
        "    next_token, done, state_zero = self.decoder.get_initial_state(context)\n",
        "    # state = state\n",
        "    state =[state_zero,state_zero]\n",
        "    for _ in range(max_length):\n",
        "        # Generate the next token\n",
        "        next_token, done, state = self.decoder.get_next_token(\n",
        "                next_token, context, done,  state, temperature)\n",
        "\n",
        "        # Collect the generated tokens\n",
        "        tokens.append(next_token)\n",
        "        # attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "            break\n",
        "\n",
        "    # Stack the lists of tokens and attention weights.\n",
        "    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "    # self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "    result = self.decoder.tokens_to_text(tokens)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f7709f55-9f9f-4d37-b071-975678c5e045",
      "metadata": {
        "id": "f7709f55-9f9f-4d37-b071-975678c5e045",
        "outputId": "89bb3cdc-3362-42a6-a3a8-0d71f10f5a56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 : i dont know . \n",
            "  1 : i dont know . \n",
            "  2 : i dont know . \n",
            "  3 : i dont know . \n",
            "  4 : i dont know . \n",
            "  5 : i dont know what about that , i dont give it a little . \n",
            "  6 : you dont talk me ? \n",
            "  7 : you think ? \n",
            "  8 : its just . \n",
            "  9 : well , yes you know what ? \n",
            " 10 : that ring dont hear with a kick on ? \n",
            " 11 : your lord something better i want anything truth once sure we look straight there three days . \n",
            " 12 : goodness that to i wanted you to get his funny paradise down within . theyve . \n"
          ]
        }
      ],
      "source": [
        "for  i in range(0, 13):\n",
        "\n",
        "    result = model.reply(['how was your date with john ?'], temperature = i/10)\n",
        "    print(f'{i:3} : {result.numpy()[0].decode()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c4edd67c-a52a-4dae-97ad-7492a234d26b",
      "metadata": {
        "id": "c4edd67c-a52a-4dae-97ad-7492a234d26b",
        "outputId": "de3136e5-563c-43f9-f061-8577c4477078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(221282, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('./dataset.csv', sep = '\\t', encoding='latin1', names = ['col1','col2']) \n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3cc8e7f2-b6fe-4782-941f-dfffb8e060a0",
      "metadata": {
        "id": "3cc8e7f2-b6fe-4782-941f-dfffb8e060a0",
        "outputId": "f27d400f-7c0c-47e3-a156-899f49664d1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input     :  No!\n",
            "Output    :  Is it radical for the wealth of this country to be turned back to the people who built the country?\n",
            "Predicted :  i remember . \n",
            "\n",
            "Input     :  Oh yeah, since when?\n",
            "Output    :  Since we learned about mental illness, paranoia, schizophrenia. All the things they taught me in Harvard. Mrs. MacNeil since the day I joined the Jesuits, I've never met one priest who has performed an exorcism, not one.\n",
            "Predicted :  no . \n",
            "\n",
            "Input     :  It's very simple. She belongs in that rarefied atmosphere of Park Avenue, expensive restaurants, and literary cocktail parties.\n",
            "Output    :  People with sense can belong wherever they're put.\n",
            "Predicted :  . \n",
            "\n",
            "Input     :  Cover me, Kid, while I mount.\n",
            "Output    :  I can't see 'em.\n",
            "Predicted :  no . \n",
            "\n",
            "Input     :  Believe me, I know your feelings on the matter.\n",
            "Output    :  The receptionist said you called earlier about something.\n",
            "Predicted :  i was the same of yours . \n",
            "\n",
            "Input     :  We're taking a different way home.\n",
            "Output    :  That's right.\n",
            "Predicted :  what ? \n",
            "\n",
            "Input     :  Lance Johnson? The surfer?\n",
            "Output    :  That's right, sir.\n",
            "Predicted :  well , if you do , and i think i mean to bring him . \n",
            "\n",
            "Input     :  We have to ... we ... we have to keep you under wraps.  Please don't, don't discuss ...\n",
            "Output    :  I understand.\n",
            "Predicted :  i dont care . \n",
            "\n",
            "Input     :  That's a chance we have to take.\n",
            "Output    :  Maybe we ought to leave.\n",
            "Predicted :  i didnt feel about it . \n",
            "\n",
            "Input     :  What did you ask me?\n",
            "Output    :  What?\n",
            "Predicted :  yes , ill you . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_num = 10\n",
        "for i in range(test_num):\n",
        "    a = data.sample(1)\n",
        "    print('Input     : ',a['col1'].values[0])\n",
        "    print('Output    : ',a['col2'].values[0])\n",
        "    result = model.reply(a['col1'].values, temperature = .6)\n",
        "    print('Predicted : ',    result.numpy()[0].decode())\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Export(tf.Module):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "    def reply(self, inputs):\n",
        "        return self.model.reply(inputs)"
      ],
      "metadata": {
        "id": "1g3o2_t2H8X2"
      },
      "execution_count": 35,
      "outputs": [],
      "id": "1g3o2_t2H8X2"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [\n",
        "    \"It's really cold here.\",\n",
        "    \"This is my life.\",\n",
        "    \"His room is a mess\"\n",
        "]"
      ],
      "metadata": {
        "id": "kQ3wDZ5PIU1e"
      },
      "execution_count": 36,
      "outputs": [],
      "id": "kQ3wDZ5PIU1e"
    },
    {
      "cell_type": "code",
      "source": [
        "export = Export(model)\n",
        "\n",
        "_ = export.reply(tf.constant(inputs))\n"
      ],
      "metadata": {
        "id": "_IVwbVOQIHs5"
      },
      "execution_count": 37,
      "outputs": [],
      "id": "_IVwbVOQIHs5"
    },
    {
      "cell_type": "code",
      "source": [
        "result = export.reply(tf.constant(inputs))\n",
        "\n",
        "print(result[0].numpy().decode())\n",
        "print(result[1].numpy().decode())\n",
        "print(result[2].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "id": "cP7IqJGTJfRr",
        "outputId": "87a38b6f-959a-4e26-92cc-d989a34c0fa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cP7IqJGTJfRr",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i dont know .                                              \n",
            "i dont know .                                              \n",
            "i dont know .                                              \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(export, 'chatbot',\n",
        "                    signatures={'serving_default': export.reply})"
      ],
      "metadata": {
        "id": "f_g8X9azKWUi",
        "outputId": "b2c70e73-5841-4574-fe33-f81ffbc8f711",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "f_g8X9azKWUi",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, embedding_2_layer_call_fn, embedding_2_layer_call_and_return_conditional_losses, embedding_3_layer_call_fn, embedding_3_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "P_f-JwOIKZRJ",
        "outputId": "d0c85ded-dfd5-46dd-9aab-a9b8862febdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "P_f-JwOIKZRJ",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/chatbot-with-attention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp ./chatbot/ /content/drive/MyDrive/tf_model/chatbot_with_attention/ -r"
      ],
      "metadata": {
        "id": "A6N2jJTQizKP"
      },
      "id": "A6N2jJTQizKP",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "reloaded = tf.saved_model.load('chatbot')\n",
        "_ = reloaded.reply(tf.constant(inputs)) #warmup"
      ],
      "metadata": {
        "id": "mgxmT2WRLWoi",
        "outputId": "6cb0762c-d884-4681-a001-a6b50848009b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mgxmT2WRLWoi",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 28s, sys: 2.25 s, total: 1min 30s\n",
            "Wall time: 1min 29s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ],
      "metadata": {
        "id": "gLnJ2Rik5O7i"
      },
      "id": "gLnJ2Rik5O7i",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "2c7934f0-9567-4cc8-a07a-0c956220e124",
      "metadata": {
        "id": "2c7934f0-9567-4cc8-a07a-0c956220e124",
        "outputId": "3f998b06-0998-4632-ae74-9b31b989d7aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " chatbot\t\t\t    log\n",
            " components\t\t\t    model_checkpoint\n",
            "'cornell movie-dialogs corpus'\t    movie_dialogue_chatbot.ipynb\n",
            " cornell_movie_dialogs_corpus.zip   preprocessing_cornelldata.py\n",
            " dataset\t\t\t    preprocess.py\n",
            " dataset.csv\t\t\t    README.md\n",
            " embedding\t\t\t    TextFile.txt\n",
            " funcyou\t\t\t    training.ipynb\n",
            " LICENSE\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "13c09f75-88d5-4dc4-9a42-398b7a17a4d0",
      "metadata": {
        "id": "13c09f75-88d5-4dc4-9a42-398b7a17a4d0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "2b586d8e-0b41-4f2c-b20a-f6e05aa051dc",
      "metadata": {
        "id": "2b586d8e-0b41-4f2c-b20a-f6e05aa051dc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "b7447dbf-c935-4080-8230-c827a38e98d1",
      "metadata": {
        "id": "b7447dbf-c935-4080-8230-c827a38e98d1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "d52f1642-700a-42ec-900f-41c19dc9cf97",
      "metadata": {
        "id": "d52f1642-700a-42ec-900f-41c19dc9cf97"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "3f40775f-25aa-45e9-9b2c-5ef2a47bafee",
      "metadata": {
        "id": "3f40775f-25aa-45e9-9b2c-5ef2a47bafee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "d8add436-3533-41ca-b864-63a85f5a30c2",
      "metadata": {
        "id": "d8add436-3533-41ca-b864-63a85f5a30c2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "4158bb21-094d-4bd9-bf1f-8a209d980676",
      "metadata": {
        "id": "4158bb21-094d-4bd9-bf1f-8a209d980676"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "de4b7996-8c34-4c51-a707-60caf8921a84",
      "metadata": {
        "id": "de4b7996-8c34-4c51-a707-60caf8921a84"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "f366abef-cd80-42bb-b680-ad634d234eb9",
      "metadata": {
        "id": "f366abef-cd80-42bb-b680-ad634d234eb9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "38d8fb6b-0362-4a39-b06b-af08bee0cf6d",
      "metadata": {
        "id": "38d8fb6b-0362-4a39-b06b-af08bee0cf6d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "13198b71-3dcf-4218-94b1-994db06e7b89",
      "metadata": {
        "id": "13198b71-3dcf-4218-94b1-994db06e7b89"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "f98e0ed1-7a81-4793-91a1-6839b9864bd8",
      "metadata": {
        "id": "f98e0ed1-7a81-4793-91a1-6839b9864bd8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "61097607-493e-4ada-979b-492f874d0ded",
      "metadata": {
        "id": "61097607-493e-4ada-979b-492f874d0ded"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}