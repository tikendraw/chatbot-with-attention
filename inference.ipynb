{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "#       start chatting ver. 1.0          #\n",
      "##########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 07:50:46.243284: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-03-03 07:50:46.712409: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-03-03 07:50:46.726709: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 5 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Op type not registered 'NormalizeUTF8' in binary running on t. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:4199\u001b[0m, in \u001b[0;36mGraph._get_op_def\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   4198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 4199\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op_def_cache[\u001b[39mtype\u001b[39;49m]\n\u001b[1;32m   4200\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'NormalizeUTF8'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:958\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 958\u001b[0m   loader \u001b[39m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[1;32m    959\u001b[0m                   ckpt_options, options, filters)\n\u001b[1;32m    960\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:154\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_export_dir \u001b[39m=\u001b[39m export_dir\n\u001b[1;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_functions \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 154\u001b[0m     function_deserialization\u001b[39m.\u001b[39;49mload_function_def_library(\n\u001b[1;32m    155\u001b[0m         library\u001b[39m=\u001b[39;49mmeta_graph\u001b[39m.\u001b[39;49mgraph_def\u001b[39m.\u001b[39;49mlibrary,\n\u001b[1;32m    156\u001b[0m         saved_object_graph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_proto,\n\u001b[1;32m    157\u001b[0m         wrapper_function\u001b[39m=\u001b[39;49m_WrapperFunction))\n\u001b[1;32m    158\u001b[0m \u001b[39m# Store a set of all concrete functions that have been set up with\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m# captures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/saved_model/function_deserialization.py:416\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mwith\u001b[39;00m graph\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m--> 416\u001b[0m   func_graph \u001b[39m=\u001b[39m function_def_lib\u001b[39m.\u001b[39;49mfunction_def_to_graph(\n\u001b[1;32m    417\u001b[0m       fdef,\n\u001b[1;32m    418\u001b[0m       structured_input_signature\u001b[39m=\u001b[39;49mstructured_input_signature,\n\u001b[1;32m    419\u001b[0m       structured_outputs\u001b[39m=\u001b[39;49mstructured_outputs)\n\u001b[1;32m    420\u001b[0m \u001b[39m# Restores gradients for function-call ops (not the same as ops that use\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m# custom gradients)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/framework/function_def_to_graph.py:82\u001b[0m, in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, structured_input_signature, structured_outputs, input_shapes)\u001b[0m\n\u001b[1;32m     80\u001b[0m         input_shapes\u001b[39m.\u001b[39mappend(input_shape)\n\u001b[0;32m---> 82\u001b[0m graph_def, nested_to_flat_tensor_name \u001b[39m=\u001b[39m function_def_to_graph_def(\n\u001b[1;32m     83\u001b[0m     fdef, input_shapes)\n\u001b[1;32m     85\u001b[0m \u001b[39mwith\u001b[39;00m func_graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m     86\u001b[0m   \u001b[39m# Add all function nodes to the graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/framework/function_def_to_graph.py:252\u001b[0m, in \u001b[0;36mfunction_def_to_graph_def\u001b[0;34m(fdef, input_shapes)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m   op_def \u001b[39m=\u001b[39m default_graph\u001b[39m.\u001b[39;49m_get_op_def(node_def\u001b[39m.\u001b[39;49mop)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m op_def\u001b[39m.\u001b[39mattr:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:4203\u001b[0m, in \u001b[0;36mGraph._get_op_def\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   4202\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_graph\u001b[39m.\u001b[39mget() \u001b[39mas\u001b[39;00m c_graph:\n\u001b[0;32m-> 4203\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39;49mTF_GraphGetOpDef(c_graph, compat\u001b[39m.\u001b[39;49mas_bytes(\u001b[39mtype\u001b[39;49m),\n\u001b[1;32m   4204\u001b[0m                                      buf)\n\u001b[1;32m   4205\u001b[0m data \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_GetBuffer(buf)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Op type not registered 'NormalizeUTF8' in binary running on t. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mIt\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms really cold here.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThis is my life.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHis room is a mess\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[39m# tf.saved_model.LoadOptions(experimental_io_device = '/job:localhost')\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m chatbot \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49msaved_model\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mchatbot\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     26\u001b[0m tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39mLoadOptions(experimental_io_device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/job:localhost\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m _ \u001b[39m=\u001b[39m chatbot\u001b[39m.\u001b[39mreply(tf\u001b[39m.\u001b[39mconstant(inputs)) \u001b[39m#warmup\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:828\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(export_dir, os\u001b[39m.\u001b[39mPathLike):\n\u001b[1;32m    827\u001b[0m   export_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(export_dir)\n\u001b[0;32m--> 828\u001b[0m result \u001b[39m=\u001b[39m load_partial(export_dir, \u001b[39mNone\u001b[39;49;00m, tags, options)[\u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    829\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_new/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:961\u001b[0m, in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    958\u001b[0m   loader \u001b[39m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[1;32m    959\u001b[0m                   ckpt_options, options, filters)\n\u001b[1;32m    960\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 961\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    962\u001b[0m       \u001b[39mstr\u001b[39m(err) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m You may be trying to load on a different device \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    963\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mfrom the computational device. Consider setting the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mto the io_device such as \u001b[39m\u001b[39m'\u001b[39m\u001b[39m/job:localhost\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    966\u001b[0m root \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mget(\u001b[39m0\u001b[39m)\n\u001b[1;32m    967\u001b[0m root\u001b[39m.\u001b[39mgraph_debug_info \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39madjust_debug_info_func_names(debug_info)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Op type not registered 'NormalizeUTF8' in binary running on t. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"##########################################\")\n",
    "print(\"#       start chatting ver. 1.0          #\")\n",
    "print(\"##########################################\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "'''\n",
    "This code is doing the following things 1 It takes input from user and clean it \n",
    "using clean text function 2 Then it splits the text into words and then converts \n",
    "each word to index using vocab dictionary If a word doesn\\'t exist in vocab dictionary \n",
    "then we replace that with OUT token which is used for padding purpose\n",
    "- generated by stenography autopilot [ üöóüë©‚Äç‚úàÔ∏è ]\n",
    "'''\n",
    "inputs = [\n",
    "    \"It's really cold here.\",\n",
    "    \"This is my life.\",\n",
    "    \"His room is a mess\"\n",
    "]\n",
    "\n",
    "# tf.saved_model.LoadOptions(experimental_io_device = '/job:localhost')\n",
    "chatbot = tf.saved_model.load('chatbot')\n",
    "tf.saved_model.LoadOptions(experimental_io_device = '/job:localhost')\n",
    "\n",
    "_ = chatbot.reply(tf.constant(inputs)) #warmup\n",
    "\n",
    "try:\n",
    "\ttemp = .6\n",
    "\t\n",
    "\n",
    "\n",
    "\n",
    "\twhile not stop_condition :\n",
    "\t\tuser_input = input(\"you : \")\n",
    "\t\tprint(user_input)\n",
    "\t\tif user_input == '!q':\n",
    "\t\t\tprint('bye')\n",
    "\t\t\tstop_condition = True\n",
    "\t\t\texit()\n",
    "\t\t# if user_input == '!q':\n",
    "\t\t# \tprint('bye')\n",
    "\t\telse:\n",
    "\t\t\tprint(\"chatbot : \", chatbot.reply(tf.constant(user_input), temperature = temp) )\n",
    "\t\t\tprint(\"\\n\")\n",
    "\n",
    "except:\n",
    "\tprint(\"sorry didn't got you , please type again :( \")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\t...\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\t\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfedff957ea7ac7ca4066cb4591428111f6e0362a36cd08e1a6ce5802ca9d684"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
